---
title: "Bayesian State Space Model"
format: 
  html:
    html-math-method: mathjax
    echo: true
    eval: true
---


## Assessment

State Space models are much better in that they seem to catch some of the dynamics.  This is still running so these results should be considered preliminary.   Mostly the sampling of the system states was not efficiently implemented.  Results to be updated.

## Basics

```{r version}
#| code-fold: true
version
library(tidyverse)
library(lubridate)
library(truncnorm)
library(mvtnorm)
# import main functions
source("MCMC_SS_functions.R")     # Main Metropolis Hasting for vectorized state space model
```


## State Space Model:

State space models are a generalization of time series models, where the observations are driven by an underlying stochastic transition process, but may be observed with noise.


A state space formulation implies that there is some stochastic process for the mean and we observe some stochastic process.   The observation process allows for the scenario where a Bank doesn't give speeches in a given month ($\alpha = 0$) or something else comes up.  The transition process models the evolution of topic importance, whereas the observation process allows those topics to be observed with noise.  Here "with noise" implies that a central bank only gives a sample of speeches in any time period.




#### Observation process:


$$DTP_{ijt} = \alpha_{ijt} \left[X_{ijt} + e_{i,j,t}\right], \ \ \ where\ \ \ e_{ijt}\sim N(0,\sigma^2_{e,i}), \ \ \ and \ \ \ \alpha_{ijt} \sim Bernoulli(p_a)$$

<!-- The observation process has the moments: -->

<!-- $$E(DTP_{ijt}\mid X_{ijt}) = \alpha_{ijt} X_{ijt}$$ -->
To control whether or not the observation occurs we use $\alpha_{ijt}\in\{0,1\}$ where  $\alpha_{ijt} \sim Bernoulli(p_a)$.


<!-- $$var(DTP_{ijt}\mid X_{ijt}, \alpha_{ijt}=1) = \sigma^2_{\epsilon,i}$$ -->
<!-- $$var(DTP_{ijt}\mid X_{ijt}, \alpha_{ijt}=0) = 0$$ -->



#### Un-observable Transition process:

Dropping the reliance on the topic index _i_, the model transitions ahead based on the  stochastic process: 

$$X_{jt} = a_j+b_{j} * X_{jt-1} + \sum_{k\in \{1,...,J\} \setminus \{j\}}c_{jk} * X_{kt-1}+ \delta_{jt}.$$
Using matrix notation this can be handled with an appropriately defined $\Theta$ and by expanding $X_{jt-1}$ to include a column of 1s.
$$= \Theta X_{jt-1}+ \delta_{jt}, \ \ \ where\ \ \ \delta_{jt}\sim N(0,\sigma_{\delta}^2).$$

The un-observable transition process _should_ track the underlying process better than a VAR model because it splits apart the states and observations.  Both will be about the same when it comes to prediction.


Note that we already know the values of $\alpha_{jt} \in\{0,1\}$  but we will model it as $\alpha_{jt}\sim Bernoulli (p_a)$.   **In a future version**, maybe we set up $p_a$ to be time varying, allowing for topics like covid and the unprovoked invasion of Ukraine to be turned off / on.  
When we observe zero for this topic or we do not observe a speech, then we have $\alpha_{jt}=0$ and $\alpha_{jt}=1$ otherwise.


#### The Complete data likelihood:

$$P(X_{ijt},\ldots X_{ij0},DTP_{ijt},\ldots,DTP_{ij0}\mid \Theta, p_a, \sigma^2_{\epsilon,i}, \sigma^2_{\delta,i}) = P(\alpha\mid p_a)P(X_{ij0})\prod_{s=1}^t P(X_{ijs}\mid X_{ijs-1}, \Theta)P(DTP_{ijs}\mid\Theta, p_a, \sigma^2_{\epsilon,i}, \sigma^2_{\delta,i},X_{ijs})$$


#### The Observed data (marginal) likelihood:

$$P(DTP_{ijt},\ldots,DTP_{ij0},\alpha_{jt}\mid \Theta, p_a, \sigma^2_{\epsilon}, \sigma^2_{\delta}) = \int_\chi\cdots\int_\chi P(\alpha\mid p_a)P(X_{j0})\prod_{s=1}^t P(X_{js}\mid X_{js-1} \Theta)\left[P(DTP_{js}\mid\Theta, p_a, \sigma^2_{\epsilon}, \sigma^2_{\delta},X_{js}))\right]^{\alpha_{jt}}dX_{j0}\cdots dX_{jT}$$

### Frequentist Approach: 

We could use a Laplace Approximation to integrate out the nuissance parameters $X$ and the unobserved $DTP$ values to obtain the MLE for $\Theta, \alpha, \sigma^2_{\epsilon,i}, \sigma^2_{\delta,i}$.  This becomes a numerical optimization routing where the optimizer needs to handle the Laplace approximation at each iteration until the marginal likelihood is optimized.  The R library _TMB_ compiles models in C and runs quickly if you want to try it.

#### Bayesian Approach:

Bayesian tools provide easy access to non-asymptotic uncertainty estimates and readily allow for unspecified correlations between parameters. Bayesian approaches also allow for non-approximate (well, less-approximate) integration for marginal likelihoods.

Include priors on structural parameters $X_{ij0},\Theta, p_a, \sigma^2_{\epsilon,i}, \sigma^2_{\delta,i}$, then sample from the joint posterior of all structural parameters and the states (nuissance parameters).  Discard the samples from the nuissance parameters to marginalize / integrate them out leaving a sample from $P(X_{ij0},\Theta, \alpha, \sigma^2_{\epsilon,i}, \sigma^2_{\delta,i} \mid DTP_{ij1},...,DTP_{ijt})$.   The sampler may be inefficient and take a while to converge unless you get fancy.  Typically people use Sequential Monte Carlo, but I'm using a faster to code, slower to run Parallel Tempering sampler.  





## Priors

The coefficients $\theta=[a,b,c]$, process evolution noise $\sigma_{\delta}^2$ observational error, $\sigma^2_{e}$, initial unobserved process $X_{j0}$, and the probability of observation $p_a$, all need priors, of which we use these models:


$$a,b,c\overset{iid}{\sim }N(0,1)$$
$$\sigma_{\delta}^2\sim exponential(1/.01)$$
$$\sigma_{e}^2\sim exponential(1/.0001)$$
The exponential distributions are parameterized using the same convention as _R_, so that $Z\sim exponential(1/\theta)$ has $E(Z) = \theta$. The prior means for the Standard Deviations are then $\sqrt{.01} = .1$ for the observation transitions, and $\sqrt{.0001} = .01$ for the observation noise process.  These may be swapped out for an inverse gamma at some point. 

$X_{j0} \overset{iid}{\sim }N(0,1)$$
$$p_a\sim U(0,1)$$


## User Defined Options

Note that there are some switches that a user may define here.  In this code block the user defines:

1. the location and file containing the time series of topics.
2. the topic of interest
3. the countries to use
4. The first month to use.
5. deciding if the log or raw data should be used.

```{r user_defined_options}
# 1. location and filename
datafolder = "cbspeeches-main/inst/data-misc/"
datafile   = paste0(datafolder,"nmf-time-series-g20.csv")

#2. topic of interest
candidate_topics = c("inflat" ,"brexit", "covid", "cbdc", "ukrain" )
(topic_of_interest = candidate_topics[4])

#3 countries to use.  
# possible option
G7_countries = c ("Canada", "France", "Germany", "Italy", "Japan",  "United Kingdom","United States")
# possible option
CUSA = c ("Canada","United States")
# actual decision:
countries2use = G7_countries# unique(dataset$country)

#4 first month to use, 
# start of dataset
start_date = ymd("2008-11-01")
# otherwise the month before the first mention in the dataset.
if(topic_of_interest=="brexit"){
  start_date = ymd("2016-02-01")
}
if(topic_of_interest=="covid"){
  start_date = ymd("2020-01-01")
}
if(topic_of_interest=="cbdc"){
  start_date = ymd("2016-02-01")
}
if(topic_of_interest=="ukrain"){
  start_date = ymd("2014-03-01")
}

#5  prefix for naming output filenames
prefix = "raw" 

```




## Data Loading:


```{r dataload}
#| warning: false

#. just loading and maybe transforming the data:
data_full = dataload(datafile, topic_of_interest, countries2use, start_date)
dates_in_use = data_full$dates_in_use
if(prefix == "log"){
  data         = log(data_full$data)
}else{
  data = data_full$data
}
```




## Sampling Scheme

Use Metropolis Hastings.  This will be slow to sample.  In future move to a Sequential Monte Carlo variant and / or sample $p_a, \sigma^2_\delta, \sigma^2_e$ directly using Gibbs steps. 

1. **sample parameters in blocks** $a$ using MH.
2. **sample parameters in blocks** $b$ using MH.
3. **sample parameters in blocks** $c$ using MH.  We expect correlations beteen country effects, so these are sampled using multivariate Normal.
4. **sample** $X_{0}$. We expect correlations between starting points for latent states, so these are sampled using multivariate Normal.
5. Loop through time points and **sample** $X_{t}$. We expect correlations between starting points for latent states, so these are sampled using multivariate Normal. Eventually we should move to doing the whole thing using SMC.
6. **sample** $p_a$ using MH.
7. **sample** $\sigma^2_\delta$ using MH.
8. **sample** $\sigma^2_e$ using MH.



## MCMC 


Set up the iterations, transition variances and frequency of tuning thereof, temperatures for running the tempering.

```{r preamble}
#| code-fold: true

if(!exists("niter")){ niter = 50000 }    # total number of iterations
print(niter)
if(!exists("nstops")){nstops = 500  }   # number of times to assess the acceptance rate 
print(nstops)
step_var = .01      # starting transition variances for {pa,X,A,B,C,sigma2_delta,sigma2_e, Xother}
temperatures = c(.25,.5,.75,.875,1) # 5 temperatures for the MCMC
```

Run the MCMC.




```{r MCMC}
#| code-fold: true
#| eval: false


filename = paste0(prefix,"dataSS_",topic_of_interest,"_results_",paste0(countries2use, collapse = "_"),"_",niter,".Rdata")
print(filename)

P = 1 # max lag


T1 = Sys.time()

PT_results = Run_PT_SS(niter,  #MCMC iterations
                  temperatures, # temperatures for PT
                  data, # raw data with potential missing values
                  P, # max lag
                  nstops,
                  Goal_acceptance_rate = c(pa = .44,
                                           X = .23,
                                           A = .23,
                                           B = .23,
                                           C = .23,
                                           sigma2_delta = .44,
                                           sigma2_e = .44), # target sampling acceptance rates
                  filename = filename,
                  step_var = 0.1,# initial guess at transition variance can be user specified.
                  CCor = diag(1, ncol(data)*(ncol(data)-1)) # potential correlation structure for C if known 
                  )



elapsed = Sys.time() - T1 
saveRDS(PT_results$CCor, paste0("CCor_",gsub(filename, pattern = "Rdata", replacement = "rds")))
cat(paste("total compute time: ", round(as.numeric(elapsed, units = "secs"),2)," seconds"))
save.image(filename)


```


## Make some plots:

In all cases discard the part of the MCMC where chains were being adapted.



::: panel-tabset

```{r}
#| echo: false

topic_of_interest = candidate_topics[1]
filename = paste0(prefix,"dataSS_",topic_of_interest,"_results_",paste0(countries2use, collapse = "_"),".Rdata")
# filename = paste0(prefix,"dataSS_",topic_of_interest,"_results_",paste0(countries2use, collapse = "_"),"_",niter,".Rdata")
```

## Topic `r topic_of_interest`

Results loop over the topics of interest and rely on having run the above code set across different topics.

```{r top1load}
#| code-fold: true

load(filename)
print(filename)
```


```{r top1index}
#| code-fold: true
#| output: asis

niter = PT_results$theta[[1]]|> nrow()
A_index        = PT_results$theta_labels |> grep(pattern = "^A")
B_index        = PT_results$theta_labels |> grep(pattern = "^B")
C_index        = PT_results$theta_labels |> grep(pattern = "^C")
sigma2_delta_index    = PT_results$theta_labels |> grep(pattern = "sigma2_delta")
sigma2_e_index = PT_results$theta_labels |> grep(pattern = "sigma2_e")
pa_index       = PT_results$theta_labels |> grep(pattern = "^pa")
X0_index       = PT_results$theta_labels |> grep(pattern = "^X0")
  
  
Nsam = 1000

NChain = length(temperatures)
datafull = rep(list(matrix(NA, nrow = Nsam, ncol = ncol(PT_results$Xmat[[1]])/length(A_index))), ncol(data))
logpost_samp = NA
inds_keep = sample(floor(niter/2):niter, size = Nsam)

for(sample_use in 1:Nsam){
  prop_model = matrix(PT_results$Xmat[[5]][inds_keep[sample_use],], ncol = length(countries2use))
  for(variable in 1:ncol(data)){
      datafull[[variable]][sample_use, ] = prop_model[,variable]
    }
}


for(country in 1:length(countries2use)){
matplot(x = dates_in_use,
  t(datafull[[country]]), 
        type = 'l',  
        main = paste("posterior predictive fit for country",
                                                          colnames(data)[country]),
  ylim = c(-.1,.5))
points(x = dates_in_use,
       data[,country], 
       pch = "*", cex = 3, col= "red")
}

```



If a topic is non-stationary over this time duration then time series will aim for the wilder behaviour and consider it typical over the calmer time frame. Time series will really just be honest about uncertainty with the goal of making predictions at the expense of modelling inference.



```{r top1logpost}
#| code-fold: true
#| output: asis

#Check sampling:

# logposterior
LP = data.frame(logpost = unlist(PT_results$log_posterior),
   temp = as.character(rep(temperatures, each = niter)),
        index = rep(1:niter,  length(temperatures)))|> filter(index>floor(niter/50))
LP|>
  filter(index  %% 5 ==0) |>
  ggplot()+
  geom_line(aes(y = logpost, x = index, color = temp), alpha = 1)+
  ggtitle("Log unnormalized posterior")


```



```{r}
#| code-fold: true
#| output: asis

# sigma2

print("sigma2_delta")
PT_results$theta[[which(temperatures ==1)]][,"sigma2_delta"]|> summary()
print("sigma2_e")
PT_results$theta[[which(temperatures ==1)]][,"sigma2_e"]|> summary()


LP = data.frame(
        sigma2_delta = PT_results$theta[[5]][,"sigma2_delta"],
        sigma2_e = PT_results$theta[[5]][,"sigma2_e"],
        index = rep(1:niter,  length(temperatures)))|> 
  filter(index>floor(niter/2)) |>
  pivot_longer(cols = starts_with("sigma2"), names_to = "sigma2", values_to = "value")
LP|>
  ggplot()+
  geom_density(aes(x = value, color = sigma2))+
  facet_wrap(~sigma2, scales = "free")


```


If there is a signal conducive to the way the model is set up then these posterior distributions should be away from zero. However, if a parameter is greater than zero it implies that the dataset is growing and is therefore not stable.


```{r top1theta}
#| code-fold: true
#| output: asis

# theta
LP =
  lapply(PT_results$theta, function(x){x|> 
      as_tibble()|>
      pivot_longer(everything(),names_to = "par", values_to = "val")|>
  mutate(index = rep(1:niter, each = ncol(x)))
    })

stacked = NULL
for(chain in 1:length(temperatures)){
  stacked = stacked |> 
    bind_rows(LP[[chain]] |> 
                mutate(temp = as.factor(temperatures[chain])))
}
stacked |> 
  filter(index>floor(niter/2))|>
  filter(str_detect(par, pattern = "^A" ))|>
  filter(temp == "1")|>
  ggplot()+
  geom_density(aes(x = val ))+
  geom_vline(xintercept = 0, linewidth = .5)+
  facet_wrap(~par) + ggtitle("thetas, A")


stacked |> 
    filter(index>floor(niter/2))|>
    filter(str_detect(par, pattern = "^B" ))|>
  filter(temp == "1")|>
    ggplot()+
    geom_density(aes(x = val ))+
    geom_vline(xintercept = 0, linewidth = .5)+
    facet_wrap(~par) + ggtitle("thetas, B")


stacked |> 
    filter(index>floor(niter/2))|>
    filter(str_detect(par, pattern = "^C" ))|>
  filter(temp == "1")|>
    ggplot()+
    geom_density(aes(x = val))+
    geom_vline(xintercept = 0)+ 
  facet_wrap(~par) + ggtitle("thetas, C")



```



```{r top2start}
#| echo: false

topic_of_interest = candidate_topics[2]
filename = paste0(prefix,"dataSS_",topic_of_interest,"_results_",paste0(countries2use, collapse = "_"),".Rdata")
# filename = paste0(prefix,"dataSS_",topic_of_interest,"_results_",paste0(countries2use, collapse = "_"),"_",niter,".Rdata")
```

## Topic `r topic_of_interest`

Results loop over the topics of interest and rely on having run the above code set across different topics.

```{r top2load}
#| code-fold: true
print(filename)
load(filename)

```


```{r top2index}
#| code-fold: true
#| output: asis

niter = PT_results$theta[[1]]|> nrow()
A_index        = PT_results$theta_labels |> grep(pattern = "^A")
B_index        = PT_results$theta_labels |> grep(pattern = "^B")
C_index        = PT_results$theta_labels |> grep(pattern = "^C")
sigma2_delta_index    = PT_results$theta_labels |> grep(pattern = "sigma2_delta")
sigma2_e_index = PT_results$theta_labels |> grep(pattern = "sigma2_e")
pa_index       = PT_results$theta_labels |> grep(pattern = "^pa")
X0_index       = PT_results$theta_labels |> grep(pattern = "^X0")
  
  
Nsam = 1000

NChain = length(temperatures)
datafull = rep(list(matrix(NA, nrow = Nsam, ncol = ncol(PT_results$Xmat[[1]])/length(A_index))), ncol(data))
logpost_samp = NA
inds_keep = sample(floor(niter/2):niter, size = Nsam)

for(sample_use in 1:Nsam){
  prop_model = matrix(PT_results$Xmat[[5]][inds_keep[sample_use],], ncol = length(countries2use))
  for(variable in 1:ncol(data)){
      datafull[[variable]][sample_use, ] = prop_model[,variable]
    }
}


for(country in 1:length(countries2use)){
matplot(x = dates_in_use,
  t(datafull[[country]]), 
        type = 'l',  
        main = paste("posterior predictive fit for country",
                                                          colnames(data)[country]),
  ylim = c(-.1,.5))
points(x = dates_in_use,
       data[,country], 
       pch = "*", cex = 3, col= "red")
}

```



If a topic is non-stationary over this time duration then time series will aim for the wilder behaviour and consider it typical over the calmer time frame. Time series will really just be honest about uncertainty with the goal of making predictions at the expense of modelling inference.



```{r top2logpost}
#| code-fold: true
#| output: asis

#Check sampling:

# logposterior
LP = data.frame(logpost = unlist(PT_results$log_posterior),
   temp = as.character(rep(temperatures, each = niter)),
        index = rep(1:niter,  length(temperatures)))|> filter(index>floor(niter/50))
LP|>
  filter(index  %% 5 ==0) |>
  ggplot()+
  geom_line(aes(y = logpost, x = index, color = temp), alpha = 1)+
  ggtitle("Log unnormalized posterior")


```



```{r}
#| code-fold: true
#| output: asis

# sigma2

print("sigma2_delta")
PT_results$theta[[which(temperatures ==1)]][,"sigma2_delta"]|> summary()
print("sigma2_e")
PT_results$theta[[which(temperatures ==1)]][,"sigma2_e"]|> summary()


LP = data.frame(
        sigma2_delta = PT_results$theta[[5]][,"sigma2_delta"],
        sigma2_e = PT_results$theta[[5]][,"sigma2_e"],
        index = rep(1:niter,  length(temperatures)))|> 
  filter(index>floor(niter/2)) |>
  pivot_longer(cols = starts_with("sigma2"), names_to = "sigma2", values_to = "value")
LP|>
  ggplot()+
  geom_density(aes(x = value, color = sigma2))+
  facet_wrap(~sigma2, scales = "free")


```


If there is a signal conducive to the way the model is set up then these posterior distributions should be away from zero. However, if a parameter is greater than zero it implies that the dataset is growing and is therefore not stable.


```{r top2theta}
#| code-fold: true
#| output: asis

# theta
LP =
  lapply(PT_results$theta, function(x){x|> 
      as_tibble()|>
      pivot_longer(everything(),names_to = "par", values_to = "val")|>
  mutate(index = rep(1:niter, each = ncol(x)))
    })

stacked = NULL
for(chain in 1:length(temperatures)){
  stacked = stacked |> 
    bind_rows(LP[[chain]] |> 
                mutate(temp = as.factor(temperatures[chain])))
}
stacked |> 
  filter(index>floor(niter/2))|>
  filter(str_detect(par, pattern = "^A" ))|>
  filter(temp == "1")|>
  ggplot()+
  geom_density(aes(x = val ))+
  geom_vline(xintercept = 0, linewidth = .5)+
  facet_wrap(~par) + ggtitle("thetas, A")


stacked |> 
    filter(index>floor(niter/2))|>
    filter(str_detect(par, pattern = "^B" ))|>
  filter(temp == "1")|>
    ggplot()+
    geom_density(aes(x = val ))+
    geom_vline(xintercept = 0, linewidth = .5)+
    facet_wrap(~par) + ggtitle("thetas, B")


stacked |> 
    filter(index>floor(niter/2))|>
    filter(str_detect(par, pattern = "^C" ))|>
  filter(temp == "1")|>
    ggplot()+
    geom_density(aes(x = val))+
    geom_vline(xintercept = 0)+ 
  facet_wrap(~par) + ggtitle("thetas, C")



```







```{r top3start}
#| echo: false

topic_of_interest = candidate_topics[3]
filename = paste0(prefix,"dataSS_",topic_of_interest,"_results_",paste0(countries2use, collapse = "_"),".Rdata")
# filename = paste0(prefix,"dataSS_",topic_of_interest,"_results_",paste0(countries2use, collapse = "_"),"_",niter,".Rdata")
```

## Topic `r topic_of_interest`

Results loop over the topics of interest and rely on having run the above code set across different topics.

```{r top3load}
#| code-fold: true
print(filename)
load(filename)

```


```{r top3index}
#| code-fold: true
#| output: asis

niter = PT_results$theta[[1]]|> nrow()
A_index        = PT_results$theta_labels |> grep(pattern = "^A")
B_index        = PT_results$theta_labels |> grep(pattern = "^B")
C_index        = PT_results$theta_labels |> grep(pattern = "^C")
sigma2_delta_index    = PT_results$theta_labels |> grep(pattern = "sigma2_delta")
sigma2_e_index = PT_results$theta_labels |> grep(pattern = "sigma2_e")
pa_index       = PT_results$theta_labels |> grep(pattern = "^pa")
X0_index       = PT_results$theta_labels |> grep(pattern = "^X0")
  
  
Nsam = 1000

NChain = length(temperatures)
datafull = rep(list(matrix(NA, nrow = Nsam, ncol = ncol(PT_results$Xmat[[1]])/length(A_index))), ncol(data))
logpost_samp = NA
inds_keep = sample(floor(niter/2):niter, size = Nsam)

for(sample_use in 1:Nsam){
  prop_model = matrix(PT_results$Xmat[[5]][inds_keep[sample_use],], ncol = length(countries2use))
  for(variable in 1:ncol(data)){
      datafull[[variable]][sample_use, ] = prop_model[,variable]
    }
}


for(country in 1:length(countries2use)){
matplot(x = dates_in_use,
  t(datafull[[country]]), 
        type = 'l',  
        main = paste("posterior predictive fit for country",
                                                          colnames(data)[country]),
  ylim = c(-.1,.5))
points(x = dates_in_use,
       data[,country], 
       pch = "*", cex = 3, col= "red")
}

```



If a topic is non-stationary over this time duration then time series will aim for the wilder behaviour and consider it typical over the calmer time frame. Time series will really just be honest about uncertainty with the goal of making predictions at the expense of modelling inference.



```{r top3logpost}
#| code-fold: true
#| output: asis

#Check sampling:

# logposterior
LP = data.frame(logpost = unlist(PT_results$log_posterior),
   temp = as.character(rep(temperatures, each = niter)),
        index = rep(1:niter,  length(temperatures)))|> filter(index>floor(niter/50))
LP|>
  filter(index  %% 5 ==0) |>
  ggplot()+
  geom_line(aes(y = logpost, x = index, color = temp), alpha = 1)+
  ggtitle("Log unnormalized posterior")


```



```{r}
#| code-fold: true
#| output: asis

# sigma2

print("sigma2_delta")
PT_results$theta[[which(temperatures ==1)]][,"sigma2_delta"]|> summary()
print("sigma2_e")
PT_results$theta[[which(temperatures ==1)]][,"sigma2_e"]|> summary()


LP = data.frame(
        sigma2_delta = PT_results$theta[[5]][,"sigma2_delta"],
        sigma2_e = PT_results$theta[[5]][,"sigma2_e"],
        index = rep(1:niter,  length(temperatures)))|> 
  filter(index>floor(niter/2)) |>
  pivot_longer(cols = starts_with("sigma2"), names_to = "sigma2", values_to = "value")
LP|>
  ggplot()+
  geom_density(aes(x = value, color = sigma2))+
  facet_wrap(~sigma2, scales = "free")


```


If there is a signal conducive to the way the model is set up then these posterior distributions should be away from zero. However, if a parameter is greater than zero it implies that the dataset is growing and is therefore not stable.


```{r top3theta}
#| code-fold: true
#| output: asis

# theta
LP =
  lapply(PT_results$theta, function(x){x|> 
      as_tibble()|>
      pivot_longer(everything(),names_to = "par", values_to = "val")|>
  mutate(index = rep(1:niter, each = ncol(x)))
    })

stacked = NULL
for(chain in 1:length(temperatures)){
  stacked = stacked |> 
    bind_rows(LP[[chain]] |> 
                mutate(temp = as.factor(temperatures[chain])))
}
stacked |> 
  filter(index>floor(niter/2))|>
  filter(str_detect(par, pattern = "^A" ))|>
  filter(temp == "1")|>
  ggplot()+
  geom_density(aes(x = val ))+
  geom_vline(xintercept = 0, linewidth = .5)+
  facet_wrap(~par) + ggtitle("thetas, A")


stacked |> 
    filter(index>floor(niter/2))|>
    filter(str_detect(par, pattern = "^B" ))|>
  filter(temp == "1")|>
    ggplot()+
    geom_density(aes(x = val ))+
    geom_vline(xintercept = 0, linewidth = .5)+
    facet_wrap(~par) + ggtitle("thetas, B")


stacked |> 
    filter(index>floor(niter/2))|>
    filter(str_detect(par, pattern = "^C" ))|>
  filter(temp == "1")|>
    ggplot()+
    geom_density(aes(x = val))+
    geom_vline(xintercept = 0)+ 
  facet_wrap(~par) + ggtitle("thetas, C")



```













```{r top4start}
#| echo: false

topic_of_interest = candidate_topics[4]
filename = paste0(prefix,"dataSS_",topic_of_interest,"_results_",paste0(countries2use, collapse = "_"),".Rdata")
# filename = paste0(prefix,"dataSS_",topic_of_interest,"_results_",paste0(countries2use, collapse = "_"),"_",niter,".Rdata")
```

## Topic `r topic_of_interest`

Results loop over the topics of interest and rely on having run the above code set across different topics.

```{r top4load}
#| code-fold: true
print(filename)
load(filename)

```


```{r top4index}
#| code-fold: true
#| output: asis

niter = PT_results$theta[[1]]|> nrow()
A_index        = PT_results$theta_labels |> grep(pattern = "^A")
B_index        = PT_results$theta_labels |> grep(pattern = "^B")
C_index        = PT_results$theta_labels |> grep(pattern = "^C")
sigma2_delta_index    = PT_results$theta_labels |> grep(pattern = "sigma2_delta")
sigma2_e_index = PT_results$theta_labels |> grep(pattern = "sigma2_e")
pa_index       = PT_results$theta_labels |> grep(pattern = "^pa")
X0_index       = PT_results$theta_labels |> grep(pattern = "^X0")
  
  
Nsam = 1000

NChain = length(temperatures)
datafull = rep(list(matrix(NA, nrow = Nsam, ncol = ncol(PT_results$Xmat[[1]])/length(A_index))), ncol(data))
logpost_samp = NA
inds_keep = sample(floor(niter/2):niter, size = Nsam)

for(sample_use in 1:Nsam){
  prop_model = matrix(PT_results$Xmat[[5]][inds_keep[sample_use],], ncol = length(countries2use))
  for(variable in 1:ncol(data)){
      datafull[[variable]][sample_use, ] = prop_model[,variable]
    }
}


for(country in 1:length(countries2use)){
matplot(x = dates_in_use,
  t(datafull[[country]]), 
        type = 'l',  
        main = paste("posterior predictive fit for country",
                                                          colnames(data)[country]),
  ylim = c(-.1,.5))
points(x = dates_in_use,
       data[,country], 
       pch = "*", cex = 3, col= "red")
}

```



If a topic is non-stationary over this time duration then time series will aim for the wilder behaviour and consider it typical over the calmer time frame. Time series will really just be honest about uncertainty with the goal of making predictions at the expense of modelling inference.



```{r top4logpost}
#| code-fold: true
#| output: asis

#Check sampling:

# logposterior
LP = data.frame(logpost = unlist(PT_results$log_posterior),
   temp = as.character(rep(temperatures, each = niter)),
        index = rep(1:niter,  length(temperatures)))|> filter(index>floor(niter/50))
LP|>
  filter(index  %% 5 ==0) |>
  ggplot()+
  geom_line(aes(y = logpost, x = index, color = temp), alpha = 1)+
  ggtitle("Log unnormalized posterior")


```



```{r}
#| code-fold: true
#| output: asis

# sigma2

print("sigma2_delta")
PT_results$theta[[which(temperatures ==1)]][,"sigma2_delta"]|> summary()
print("sigma2_e")
PT_results$theta[[which(temperatures ==1)]][,"sigma2_e"]|> summary()


LP = data.frame(
        sigma2_delta = PT_results$theta[[5]][,"sigma2_delta"],
        sigma2_e = PT_results$theta[[5]][,"sigma2_e"],
        index = rep(1:niter,  length(temperatures)))|> 
  filter(index>floor(niter/2)) |>
  pivot_longer(cols = starts_with("sigma2"), names_to = "sigma2", values_to = "value")
LP|>
  ggplot()+
  geom_density(aes(x = value, color = sigma2))+
  facet_wrap(~sigma2, scales = "free")


```


If there is a signal conducive to the way the model is set up then these posterior distributions should be away from zero. However, if a parameter is greater than zero it implies that the dataset is growing and is therefore not stable.


```{r top4theta}
#| code-fold: true
#| output: asis

# theta
LP =
  lapply(PT_results$theta, function(x){x|> 
      as_tibble()|>
      pivot_longer(everything(),names_to = "par", values_to = "val")|>
  mutate(index = rep(1:niter, each = ncol(x)))
    })

stacked = NULL
for(chain in 1:length(temperatures)){
  stacked = stacked |> 
    bind_rows(LP[[chain]] |> 
                mutate(temp = as.factor(temperatures[chain])))
}
stacked |> 
  filter(index>floor(niter/2))|>
  filter(str_detect(par, pattern = "^A" ))|>
  filter(temp == "1")|>
  ggplot()+
  geom_density(aes(x = val ))+
  geom_vline(xintercept = 0, linewidth = .5)+
  facet_wrap(~par) + ggtitle("thetas, A")


stacked |> 
    filter(index>floor(niter/2))|>
    filter(str_detect(par, pattern = "^B" ))|>
  filter(temp == "1")|>
    ggplot()+
    geom_density(aes(x = val ))+
    geom_vline(xintercept = 0, linewidth = .5)+
    facet_wrap(~par) + ggtitle("thetas, B")


stacked |> 
    filter(index>floor(niter/2))|>
    filter(str_detect(par, pattern = "^C" ))|>
  filter(temp == "1")|>
    ggplot()+
    geom_density(aes(x = val))+
    geom_vline(xintercept = 0)+ 
  facet_wrap(~par) + ggtitle("thetas, C")



```


















```{r top5start}
#| echo: false

topic_of_interest = candidate_topics[5]
filename = paste0(prefix,"dataSS_",topic_of_interest,"_results_",paste0(countries2use, collapse = "_"),".Rdata")
# filename = paste0(prefix,"dataSS_",topic_of_interest,"_results_",paste0(countries2use, collapse = "_"),"_",niter,".Rdata")
```

## Topic `r topic_of_interest`

Results loop over the topics of interest and rely on having run the above code set across different topics.

```{r top5load}
#| code-fold: true
print(filename)
load(filename)

```


```{r top5index}
#| code-fold: true
#| output: asis

niter = PT_results$theta[[1]]|> nrow()
A_index        = PT_results$theta_labels |> grep(pattern = "^A")
B_index        = PT_results$theta_labels |> grep(pattern = "^B")
C_index        = PT_results$theta_labels |> grep(pattern = "^C")
sigma2_delta_index    = PT_results$theta_labels |> grep(pattern = "sigma2_delta")
sigma2_e_index = PT_results$theta_labels |> grep(pattern = "sigma2_e")
pa_index       = PT_results$theta_labels |> grep(pattern = "^pa")
X0_index       = PT_results$theta_labels |> grep(pattern = "^X0")
  
  
Nsam = 1000

NChain = length(temperatures)
datafull = rep(list(matrix(NA, nrow = Nsam, ncol = ncol(PT_results$Xmat[[1]])/length(A_index))), ncol(data))
logpost_samp = NA
inds_keep = sample(floor(niter/2):niter, size = Nsam)

for(sample_use in 1:Nsam){
  prop_model = matrix(PT_results$Xmat[[5]][inds_keep[sample_use],], ncol = length(countries2use))
  for(variable in 1:ncol(data)){
      datafull[[variable]][sample_use, ] = prop_model[,variable]
    }
}


for(country in 1:length(countries2use)){
matplot(x = dates_in_use,
  t(datafull[[country]]), 
        type = 'l',  
        main = paste("posterior predictive fit for country",
                                                          colnames(data)[country]),
  ylim = c(-.1,.5))
points(x = dates_in_use,
       data[,country], 
       pch = "*", cex = 3, col= "red")
}

```



If a topic is non-stationary over this time duration then time series will aim for the wilder behaviour and consider it typical over the calmer time frame. Time series will really just be honest about uncertainty with the goal of making predictions at the expense of modelling inference.



```{r top5logpost}
#| code-fold: true
#| output: asis

#Check sampling:

# logposterior
LP = data.frame(logpost = unlist(PT_results$log_posterior),
   temp = as.character(rep(temperatures, each = niter)),
        index = rep(1:niter,  length(temperatures)))|> filter(index>floor(niter/50))
LP|>
  filter(index  %% 5 ==0) |>
  ggplot()+
  geom_line(aes(y = logpost, x = index, color = temp), alpha = 1)+
  ggtitle("Log unnormalized posterior")


```



```{r}
#| code-fold: true
#| output: asis

# sigma2

print("sigma2_delta")
PT_results$theta[[which(temperatures ==1)]][,"sigma2_delta"]|> summary()
print("sigma2_e")
PT_results$theta[[which(temperatures ==1)]][,"sigma2_e"]|> summary()


LP = data.frame(
        sigma2_delta = PT_results$theta[[5]][,"sigma2_delta"],
        sigma2_e = PT_results$theta[[5]][,"sigma2_e"],
        index = rep(1:niter,  length(temperatures)))|> 
  filter(index>floor(niter/2)) |>
  pivot_longer(cols = starts_with("sigma2"), names_to = "sigma2", values_to = "value")
LP|>
  ggplot()+
  geom_density(aes(x = value, color = sigma2))+
  facet_wrap(~sigma2, scales = "free")


```


If there is a signal conducive to the way the model is set up then these posterior distributions should be away from zero. However, if a parameter is greater than zero it implies that the dataset is growing and is therefore not stable.


```{r top5theta}
#| code-fold: true
#| output: asis

# theta
LP =
  lapply(PT_results$theta, function(x){x|> 
      as_tibble()|>
      pivot_longer(everything(),names_to = "par", values_to = "val")|>
  mutate(index = rep(1:niter, each = ncol(x)))
    })

stacked = NULL
for(chain in 1:length(temperatures)){
  stacked = stacked |> 
    bind_rows(LP[[chain]] |> 
                mutate(temp = as.factor(temperatures[chain])))
}
stacked |> 
  filter(index>floor(niter/2))|>
  filter(str_detect(par, pattern = "^A" ))|>
  filter(temp == "1")|>
  ggplot()+
  geom_density(aes(x = val ))+
  geom_vline(xintercept = 0, linewidth = .5)+
  facet_wrap(~par) + ggtitle("thetas, A")


stacked |> 
    filter(index>floor(niter/2))|>
    filter(str_detect(par, pattern = "^B" ))|>
  filter(temp == "1")|>
    ggplot()+
    geom_density(aes(x = val ))+
    geom_vline(xintercept = 0, linewidth = .5)+
    facet_wrap(~par) + ggtitle("thetas, B")


stacked |> 
    filter(index>floor(niter/2))|>
    filter(str_detect(par, pattern = "^C" ))|>
  filter(temp == "1")|>
    ggplot()+
    geom_density(aes(x = val))+
    geom_vline(xintercept = 0)+ 
  facet_wrap(~par) + ggtitle("thetas, C")



```

:::


## Running this on real data:

- Expect the code to take around 8 hours for 50k iterations.  Yes my code could be made more efficient using Sequential Monte Carlo. This is a more appropriate strategy, but parallel tempering is notably difficult to break.  Futhermore, if left to run for a really long time,  Metropolis Hastings types of algorithms are guaranteed to converge to the target posterior under mmild conditions. 
- Another reasonable solution is to go fully frequentist and optimize the marginal likelihood by integrating over the nuissance states **X** using Integrated Nested Laplace Approximation.  The package **tmb** is designed to do this quickly.
- Use the script to extract the R code, because this will take a while to run. Note that the script will need to be adjusted because this quarto file generates some fake data.

```{r purl}
#| eval: false

knitr::purl("Data_PTmodelling_state_space.qmd")


```
