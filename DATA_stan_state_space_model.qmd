---
title: "Bayesian State Space Model in Stan"
format: 
  html:
    html-math-method: mathjax
    echo: true
    eval: true
---

<!-- I'm not sure why but the process noise sigma_proc is sampling some negative values.  The stan file should prevent that.  -->

## Assessment

State Space models are much better in that they seem to catch some of the dynamics.  HMC should be a good tool, since it uses some information about the shape of the posterior.  However the parameters and states are highly correlated making them difficult to sample.


#### Other things to try

- Sequential monte carlo which gets around the complex posterior by sequentially adding one time point at each stage, then filtering and repopulating samples.
- Sequentially constrained Monte Carlo.  Similar to SMC, but rather than adding one data point at each stage, use the full dataset at each stage, but slowly bring in the model as a constraint.
- Approximately marginalizing over the latent states using a Laplace Approximation.  The fastest is to use the very frequentist **TMB** software as an optimizer since it all runs in c++.  Slower is to use an Integrated Nested Laplace Approximation (INLA) which facilitates a Bayesian approach focused on the marginal posterior. 


## Basics

```{r version}
#| code-fold: true
#| message: false
#| warning: false
version
library(tidyverse)
library(rlang)
library(lubridate)
library(rstan)
library(coda) # diagnostics
library(reshape2) # reshaping matrices
library(bayesplot) # stan output plots in ggplot
options(mc.cores = 4)
rstan_options(auto_write = TRUE)

source("MCMC_SS_functions.R")     # Just used for data loading

```


## State Space Model:

State space models are a generalization of time series models, where the observations are driven by an underlying stochastic transition process, but may be observed with noise.


A state space formulation implies that there is some stochastic process for the mean and we observe some stochastic process.   The observation process allows for the scenario where a Bank doesn't give speeches in a given month ($\alpha = 0$) or something else comes up.  The transition process models the evolution of topic importance, whereas the observation process allows those topics to be observed with noise.  Here "with noise" implies that a central bank only gives a sample of speeches in any time period.




#### Observation process:

Observed Document Term Proportion $DTP_{ijt}$, for topic $i$, country $j$ at time $t$ are modelled as noisy realizations of an underlying state process $X_{ijt}$:

$$DTP_{ijt} = \alpha_{ijt} \left[X_{ijt} + e_{i,j,t}\right], \ \ \ where\ \ \ e_{ijt}\sim N(0,\sigma^2_{e,i}), \ \ \ and \ \ \ \alpha_{ijt} \{0,1\}$$
The use of $\alpha$ allows observations to be turned on / off. 


#### Un-observable Transition process:

Dropping the reliance on the topic index _i_, the model transitions ahead based on the  stochastic process: 

$$X_{jt} = a_j+b_{j} * X_{jt-1} + \sum_{k\in \{1,...,J\} \setminus \{j\}}c_{jk} * X_{kt-1}+ \delta_{jt}.$$
Using matrix notation this can be written with a $J$ by $J$ matrix $\Theta$ with diagonal elements corresponding to $b_j$ and off diagonals  $c_{jk}$, along with the $J$ by 1 vector of intercepts $A$ and vector of stochastic process noise realizations $\delta_t$:
$$X_{t}= A+\Theta X_{t-1}+ \delta_{t}, \ \ \ where\ \ \ \delta_{t}\sim MVN(0,\Sigma_{\delta}).$$
The use of **Multivariate Normal** with correlation $\Sigma_\delta$ vs something with independent elements is a modelling decision.  I don't think that it will interfere with estimation of $c_{jk}$.  The term $c_{jk}$ tracks how the past of country $k$ impacts the present of country $j$, whereas any correlation structure in $\Sigma_\delta$ relates more to external events that both banks are looking at when crafting speeches.


The un-observable transition process _should_ track the underlying process better than a VAR model because it splits apart the states and observations.  Both will be about the same when it comes to prediction.


Note that we already know the values of $\alpha_{jt} \in\{0,1\}$  but we will model it as $\alpha_{jt}\sim Bernoulli (p_a)$.   **In a future version**, maybe we set up $p_a$ to be time varying, allowing for topics like covid and the unprovoked invasion of Ukraine to be turned off / on.  
When we observe zero for this topic or we do not observe a speech, then we have $\alpha_{jt}=0$ and $\alpha_{jt}=1$ otherwise.


#### The Complete data likelihood:

$$P(X_{ijt},\ldots X_{ij0},DTP_{ijt},\ldots,DTP_{ij0}\mid \Theta, p_a, \sigma^2_{\epsilon,i}, \sigma^2_{\delta,i}) = P(\alpha\mid p_a)P(X_{ij0})\prod_{s=1}^t P(X_{ijs}\mid X_{ijs-1}, \Theta)P(DTP_{ijs}\mid\Theta, p_a, \sigma^2_{\epsilon,i}, \sigma^2_{\delta,i},X_{ijs})$$


#### The Observed data (marginal) likelihood:

$$P(DTP_{ijt},\ldots,DTP_{ij0},\alpha_{jt}\mid \Theta, p_a, \sigma^2_{\epsilon}, \sigma^2_{\delta}) = \int_\chi\cdots\int_\chi P(\alpha\mid p_a)P(X_{j0})\prod_{s=1}^t P(X_{js}\mid X_{js-1} \Theta)\left[P(DTP_{js}\mid\Theta, p_a, \sigma^2_{\epsilon}, \sigma^2_{\delta},X_{js}))\right]^{\alpha_{jt}}dX_{j0}\cdots dX_{jT}$$

### Frequentist Approach: 

We could use a Laplace Approximation to integrate out the nuissance parameters $X$ and the unobserved $DTP$ values to obtain the MLE for $\Theta, \alpha, \sigma^2_{\epsilon,i}, \sigma^2_{\delta,i}$.  This becomes a numerical optimization routing where the optimizer needs to handle the Laplace approximation at each iteration until the marginal likelihood is optimized.  The R library _TMB_ compiles models in C and runs quickly if you want to try it.

#### Bayesian Approach:

Bayesian tools provide easy access to non-asymptotic uncertainty estimates and readily allow for unspecified correlations between parameters. Bayesian approaches also allow for non-approximate (well, less-approximate) integration for marginal likelihoods.

Include priors on structural parameters $X_{ij0},\Theta, p_a, \sigma^2_{\epsilon,i}, \sigma^2_{\delta,i}$, then sample from the joint posterior of all structural parameters and the states (nuissance parameters).  Discard the samples from the nuissance parameters to marginalize / integrate them out leaving a sample from $P(X_{ij0},\Theta, \alpha, \sigma^2_{\epsilon,i}, \sigma^2_{\delta,i} \mid DTP_{ij1},...,DTP_{ijt})$.   The sampler may be inefficient and take a while to converge unless you get fancy.  Typically people use Sequential Monte Carlo, but I'm using a faster to code, slower to run Parallel Tempering sampler.  





## Priors

The coefficients $\theta=[a,b,c]$, process evolution noise $\sigma_{\delta}^2$ observational error, $\sigma^2_{e}$, initial unobserved process $X_{j0}$, and the probability of observation $p_a$, all need priors, of which we use these models:


$$a,b,c\overset{iid}{\sim }N(0,1)$$
$$\sigma_{\delta}\sim half-Normal(0,.25), \ \ with \ \ mean\ \  0.199$$

$$\sigma_{e}\sim half-Normal(0,.008), \ \ with \ \ mean \ \ 0.0064$$

$X_{j0} \overset{iid}{\sim }N(0,1)$$


## User Defined Options

Note that there are some switches that a user may define here.  In this code block the user defines:

1. the location and file containing the time series of topics.
2. the topic of interest
3. the countries to use
4. The first month to use.
5. deciding if the log or raw data should be used.

```{r user_defined_options}
# 1. location and filename
datafolder = "cbspeeches-main/inst/data-misc/"
datafile   = paste0(datafolder,"nmf-time-series-g20.csv")

#2. topic of interest
candidate_topics = c("inflat" ,"brexit", "covid", "cbdc", "ukrain" )
topic_of_interest = candidate_topics[3]

#3 countries to use.  
# possible option
G7_countries = c ("Canada", "France", "Germany", "Italy", "Japan",  "United Kingdom","United States")
# possible option
CUSA = c ("Canada","United States")
# actual decision:
countries2use = G7_countries# unique(dataset$country)

#4 first month to use, 
# start of dataset
start_date = ymd("2008-11-01")
# otherwise the month before the first mention in the dataset.
if(topic_of_interest=="brexit"){
  start_date = ymd("2016-02-01")
}
if(topic_of_interest=="covid"){
  start_date = ymd("2020-01-01")
}
if(topic_of_interest=="cbdc"){
  start_date = ymd("2016-02-01")
}
if(topic_of_interest=="ukrain"){
  start_date = ymd("2014-03-01")
}

#5  prefix for naming output filenames
prefix = "stan_" 

```




## Data Loading:


```{r dataload}
#| warning: false

#. just loading and maybe transforming the data:
data_full = dataload(datafile, topic_of_interest, countries2use, start_date)
dates_in_use = data_full$dates_in_use
if(prefix == "log"){
  data         = log(data_full$data)
}else{
  data = data_full$data
}
```






## MCMC 



Set up the iterations, transition variances and frequency of tuning thereof, temperatures for running the tempering.

```{r preamble}
#| code-fold: false
niter = 60000     # total number of iterations

```

Run the MCMC using **stan**.  Stan uses a no u-turn variant of _Hamiltonian Monte Carlo_, so this _should_ be more efficient than Metropolis Hastings.


```{r MCMC}
#| code-fold: true
#| eval: false


filename = paste0(prefix,"dataSS_",topic_of_interest,"_results_",paste0(countries2use, collapse = "_"),"_",niter,".Rdata")
print(filename)


T1 = Sys.time()

# Set up the data so that NAs and zeros are treated the same way.
data[is.na(data)] = 0
colnames(data) = NULL

# Prepare list for Stan
stan_data <- list(
  T = nrow(data),
  K = ncol(data),
  I = diag(ncol(data)),
  y_obs = data
)

# Compile and run
fit <- stan(
  file = "state_space.stan",
  data = stan_data,
  iter = niter, 
  init = "vb",
  warmup = niter/2,
  chains = 4,
  algorithm = "NUTS",
  control = list(adapt_delta   = .99,
                 max_treedepth = 30)
)

# Summarize results
print(fit, pars = c("theta"))


elapsed = Sys.time() - T1 
cat(paste("total compute time: ", round(as.numeric(elapsed, units = "hours"),2)," hours."))
save.image(filename)


```


## Make some plots:

In all cases discard the part of the MCMC where chains were being adapted.



:::: panel-tabset

```{r, cache = TRUE}
#| echo: false
topic_index_for_sections = 0
topic_index_for_sections = topic_index_for_sections+1
topic_of_interest = candidate_topics[topic_index_for_sections]
candidate_files_to_load = list.files(pattern = paste0("stan_2chain_dataSS_",topic_of_interest,"_results_Canada_France_Germany_Italy_Japan_United Kingdom_United States_60000"))
```

## Topic `r topic_of_interest`


Results loop over the topics of interest and rely on having run the above code set across different topics.

```{r chunk274}
#| code-fold: true
runtime = filesloaded = iters_run  =  rep(NA, length(candidate_files_to_load))

# clunky and lazy way to get at the number of time steps for this topic
filename = candidate_files_to_load[1]
load(filename)
niter = 60000
parnames = names(fit)

# collect the names of parameters

A_index             = parnames  |> grep(pattern = "^A")
theta_index         = parnames  |> grep(pattern = "theta")
sigma_obs_index     = parnames  |> grep(pattern = "sigma_obs")   
sigma_proc_index    = parnames  |> grep(pattern = "sigma_proc")
# Note that L_corr is the lower triangle cholesky decomposition of the correlation matrix so that the diagonals and upper triangle elements will be all zeros and are not sampled.
L_corr_index = NULL
for(rows in 2:length(A_index)){
  for(cols in 1:(rows-1)){
    L_corr_index = c(L_corr_index,
                     grep(parnames , pattern = paste0("L_corr\\[",rows,",",cols)))
  }
}
L_proc_index        = parnames  |> grep(pattern = "L_proc")
lp__index           = parnames  |> grep(pattern = "lp__")
X_index             = parnames  |> grep(pattern = "^X")

# collect the sampled parameters
# dimensions are: iterations, chains, parameters
fittedA          = array(NA, dim = c(niter,length(candidate_files_to_load),length(A_index)))
fittedtheta      = array(NA, dim = c(niter,length(candidate_files_to_load),length(theta_index)))
fittedsigma_obs  = array(NA, dim = c(niter,length(candidate_files_to_load),length(sigma_obs_index)))
fittedsigma_proc = array(NA, dim = c(niter,length(candidate_files_to_load),length(sigma_proc_index)))
fittedLcorr      = array(NA, dim = c(niter,length(candidate_files_to_load),length(L_corr_index)))
fittedLproc      = array(NA, dim = c(niter,length(candidate_files_to_load),length(L_proc_index)))
fittedlp__       = array(NA, dim = c(niter,length(candidate_files_to_load),length(lp__index)))
fittedX          = array(NA, dim = c(niter,length(candidate_files_to_load),length(X_index)))


for(lp in 1:length(candidate_files_to_load)){
  filename = candidate_files_to_load[lp]
  load(filename)
  runtime[lp]      = as.numeric(elapsed, units = "hours")
  filesloaded[lp]  = filename 
  iters_run[lp]    = niter
  
  fittedA[,lp,]          = extract(fit,permuted = FALSE, inc_warmup = TRUE, pars = parnames[A_index])
  fittedtheta[,lp,]      = extract(fit,permuted = FALSE, inc_warmup = TRUE, pars = parnames[theta_index])
  fittedsigma_obs[,lp,]  = extract(fit,permuted = FALSE, inc_warmup = TRUE, pars = parnames[sigma_obs_index])
  fittedsigma_proc[,lp,] = extract(fit,permuted = FALSE, inc_warmup = TRUE, pars = parnames[sigma_proc_index])
  fittedLcorr[,lp,]      = extract(fit,permuted = FALSE, inc_warmup = TRUE, pars = parnames[L_corr_index])
  fittedLproc[,lp,]      = extract(fit,permuted = FALSE, inc_warmup = TRUE, pars = parnames[L_proc_index])
  fittedlp__[,lp,]       = extract(fit,permuted = FALSE, inc_warmup = TRUE, pars = parnames[lp__index])
  fittedX[,lp,]          = extract(fit,permuted = FALSE, inc_warmup = TRUE, pars = parnames[X_index])
  
}

```

Diagnostics:

Total compute time: `r round(runtime,2)` hours.

As a sanity check, just make sure that all of the median log posteriors  are pretty close to each other.  If a subset are more than like 2% worse than others, consider if they are sampling from different places and/or should be discarded.

```{r}
#| cache: true

burnin = 30000 # half the iterations
niter = 60000
apply(fittedlp__[-c(1:burnin),,],2,median)

```



Consider the Gelman-Rubin Diagnostics to see if the within chain variability is similar to the between chain variability.

Make a mega matrix, but also consider little pieces later for diagnostics:

```{r}
#| code-fold: true

# reconstruct the mattrices to be easier to plot
# then slap on some names
combo_fittedA_mat           = cbind(matrix(aperm(fittedA[-c(1:burnin), , ],            c(1, 2, 3)),nrow = dim(fittedA)[1]*dim(fittedA)[2],                   ncol = dim(fittedA)[3]),          iter = rep((1+burnin):niter, dim(fittedA)[2]),         chain = rep(1:dim(fittedA)[2],           each = burnin)); colnames(combo_fittedA_mat)            = c(parnames[A_index], "iter",  "chain")
combo_fittedtheta_mat       = cbind(matrix(aperm(fittedtheta[-c(1:burnin), , ],        c(1, 2, 3)),nrow = dim(fittedtheta)[1]*dim(fittedtheta)[2],           ncol = dim(fittedtheta)[3]),      iter = rep((1+burnin):niter, dim(fittedtheta)[2]),     chain = rep(1:dim(fittedtheta)[2],       each = burnin)); colnames(combo_fittedtheta_mat)            = c(parnames[theta_index], "iter",  "chain")
combo_fittedsigma_obs_mat   = cbind(matrix(aperm(fittedsigma_obs[-c(1:burnin), , ],    c(1, 2   )),nrow = dim(fittedsigma_obs)[1]*dim(fittedsigma_obs)[2],   ncol = dim(fittedsigma_obs)[3]),  iter = rep((1+burnin):niter, dim(fittedsigma_obs)[2]), chain = rep(1:dim(fittedsigma_obs)[2],   each = burnin)); colnames(combo_fittedsigma_obs_mat)            = c(parnames[sigma_obs_index], "iter",  "chain")
combo_fittedsigma_proc_mat  = cbind(matrix(aperm(fittedsigma_proc[-c(1:burnin), , ],   c(1, 2, 3)),nrow = dim(fittedsigma_proc)[1]*dim(fittedsigma_proc)[2], ncol = dim(fittedsigma_proc)[3]), iter = rep((1+burnin):niter, dim(fittedsigma_proc)[2]),chain = rep(1:dim(fittedsigma_proc)[2],  each = burnin)); colnames(combo_fittedsigma_proc_mat)            = c(parnames[sigma_proc_index], "iter",  "chain")
combo_fittedLcorr_mat       = cbind(matrix(aperm(fittedLcorr[-c(1:burnin), , ],        c(1, 2, 3)),nrow = dim(fittedLcorr)[1]*dim(fittedLcorr)[2],           ncol = dim(fittedLcorr)[3]),      iter = rep((1+burnin):niter, dim(fittedLcorr)[2]),     chain = rep(1:dim(fittedLcorr)[2],       each = burnin)); colnames(combo_fittedLcorr_mat)            = c(parnames[L_corr_index], "iter",  "chain")
combo_fittedLproc_mat       = cbind(matrix(aperm(fittedLproc[-c(1:burnin), , ],        c(1, 2, 3)),nrow = dim(fittedLproc)[1]*dim(fittedLproc)[2],           ncol = dim(fittedLproc)[3]),      iter = rep((1+burnin):niter, dim(fittedLproc)[2]),     chain = rep(1:dim(fittedLproc)[2],       each = burnin)); colnames(combo_fittedLproc_mat)            = c(parnames[L_proc_index], "iter",  "chain")
combo_fittedlp___mat        = cbind(matrix(aperm(fittedlp__[-c(1:burnin), , ],         c(1, 2   )),nrow = dim(fittedlp__)[1]*dim(fittedlp__)[2],             ncol = dim(fittedlp__)[3]),       iter = rep((1+burnin):niter, dim(fittedlp__)[2]),      chain = rep(1:dim(fittedlp__)[2],        each = burnin)); colnames(combo_fittedlp___mat)            = c(parnames[lp__index], "iter",  "chain")
combo_fittedX_mat           = cbind(matrix(aperm(fittedX[-c(1:burnin), , ],            c(1, 2, 3)),nrow = dim(fittedX)[1]*dim(fittedX)[2],                   ncol = dim(fittedX)[3]),          iter = rep((1+burnin):niter, dim(fittedX)[2]),         chain = rep(1:dim(fittedX)[2],           each = burnin)); colnames(combo_fittedX_mat)            = c(parnames[X_index], "iter",  "chain")

```


::: {.callout-note collapse=true title="log posterior trace"}
```{r}
# do some thinning to make the traceplots plots faster
combo_fittedlp___mat[seq(1,nrow(combo_fittedlp___mat),by = 10),] |> 
  as_tibble()|>
  pivot_longer(cols = -c("iter","chain"), names_to = "parameter", values_to = "sample")|>
  ggplot(aes(x = iter,y = sample, colour = as.factor(chain)))+
  geom_line(alpha = .8)


```

:::


::: {.callout-note collapse=true title="log posterior trace"}
```{r}
# do some thinning to make the traceplots plots faster
combo_fittedlp___mat[seq(1,nrow(combo_fittedlp___mat),by = 10),] |> 
  as_tibble()|>
  pivot_longer(cols = -c("iter","chain"), names_to = "parameter", values_to = "sample")|>
  ggplot(aes(x = iter,y = sample, colour = as.factor(chain)))+
  geom_line(alpha = .8)


```

:::

::: {.callout-note collapse=true title="A"}







```{r}

# Create a list of mcmc objects, one for each chain
mcmc_chains_list <- lapply(1:dim(fittedA)[2], function(i) {
  # Extract the i-th chain (across the second dimension)
  # and convert it to an mcmc object
  mcmc(fittedA[-c(1:burnin),i,])
})
# Combine the list of mcmc objects into an mcmc.list object
mclist <- as.mcmc.list(mcmc_chains_list)
gelman.diag(mclist)

# do some heavy thinning to make the traceplots plots faster
stacked = combo_fittedA_mat[seq(1,nrow(combo_fittedA_mat), by = 100),] |> 
  as_tibble()|>
  pivot_longer(cols = -c("iter","chain"), names_to = "parameter", values_to = "sample")

for(country in 1:length(countries2use)){
  stacked = stacked|> mutate(
    parameter = str_replace_all(parameter,
                                pattern = paste0("A\\[",country),
                                replacement = paste0("A\\[",countries2use[country])))
}
stacked |>   ggplot(aes(x = iter,y = sample, colour = as.factor(chain)))+
  geom_line(alpha = .3)+
  facet_wrap(~parameter)






# do some light thinning to make the density plots faster
stacked = combo_fittedA_mat[seq(1,nrow(combo_fittedA_mat), by = 5),] |>
  as_tibble()|>
  pivot_longer(cols = -c("iter","chain"), names_to = "parameter", values_to = "sample")


for(country in 1:length(countries2use)){
  stacked = stacked|> mutate(
    parameter = str_replace_all(parameter,
                                pattern = paste0("A\\[",country),
                                replacement = paste0("A\\[",countries2use[country])))
}

stacked|>   ggplot(aes(x = sample, colour = as.factor(chain)))+
  geom_density()+
  facet_wrap(~parameter, scales = "free")


```
:::

::: {.callout-note collapse=true title="theta"}

```{r}


# Create a list of mcmc objects, one for each chain
mcmc_chains_list <- lapply(1:dim(fittedtheta)[2], function(i) {
  # Extract the i-th chain (across the second dimension)
  # and convert it to an mcmc object
  mcmc(fittedtheta[-c(1:burnin),i,])
})
# Combine the list of mcmc objects into an mcmc.list object
mclist <- as.mcmc.list(mcmc_chains_list)

gelman.diag(mclist)


# do some heavy thinning to make the traceplots plots faster
stacked = combo_fittedtheta_mat[seq(1,nrow(combo_fittedtheta_mat), by = 100),] |>  
  as_tibble()|>
  pivot_longer(cols = -c("iter","chain"), names_to = "parameter", values_to = "sample")



stacked |>   ggplot(aes(x = iter,y = sample, colour = as.factor(chain)))+
  geom_line(alpha = .3)+
  facet_wrap(~parameter)


# do some light thinning to make the density plots faster
combo_fittedtheta_mat[seq(1,nrow(combo_fittedtheta_mat), by = 5),] |>  
  as_tibble()|>
  pivot_longer(cols = -c("iter","chain"), names_to = "parameter", values_to = "sample")|>
  ggplot(aes(x = sample, colour = as.factor(chain)))+
  geom_density()+
  geom_vline(xintercept = 0)+
  facet_wrap(~parameter, scales = "free_y")

```

:::


::: {.callout-note collapse=true title="observational noise"}

The $\sigma_{\e}$ aka sigma\_obs, the observational noise:



```{r}
# Create a list of mcmc objects, one for each chain
mcmc_chains_list <- lapply(1:dim(fittedsigma_obs)[2], function(i) {
  # Extract the i-th chain (across the second dimension)
  # and convert it to an mcmc object
  mcmc(fittedsigma_obs[-c(1:burnin), i,]) 
})
# Combine the list of mcmc objects into an mcmc.list object
mclist <- as.mcmc.list(mcmc_chains_list)
gelman.diag(mclist)




# do some heavy thinning to make the traceplots plots faster
combo_fittedsigma_obs_mat[seq(1,nrow(combo_fittedsigma_obs_mat),by = 100),] |> 
  as_tibble()|>
  pivot_longer(cols = -c("iter","chain"), names_to = "parameter", values_to = "sample")|>
  ggplot(aes(x = iter,y = sample, colour = as.factor(chain)))+
  geom_line(alpha = .3)





# do some light thinning to make the density plots faster
combo_fittedsigma_obs_mat[seq(1,nrow(combo_fittedsigma_obs_mat),by = 5),] |> 
  as_tibble()|>
  pivot_longer(cols = -c("iter","chain"), names_to = "parameter", values_to = "sample")|>
  ggplot(aes(x = sample, colour = as.factor(chain)))+
  geom_density()



```
:::



::: {.callout-note collapse=true title="process noise"}

The $\sigma_{\delta}$ aka sigma\_proc.  These are the diagonal elements of the process noise.

```{r}

# Create a list of mcmc objects, one for each chain
mcmc_chains_list <- lapply(1:dim(fittedsigma_proc)[2], function(i) {
  # Extract the i-th chain (across the second dimension)
  # and convert it to an mcmc object
  mcmc(fittedsigma_proc[-c(1:burnin), i,]) 
})
# Combine the list of mcmc objects into an mcmc.list object
mclist <- as.mcmc.list(mcmc_chains_list)
gelman.diag(mclist)




# do some heavy thinning to make the traceplots plots faster
stacked = combo_fittedsigma_proc_mat[seq(1,nrow(combo_fittedsigma_proc_mat),by = 100),] |> 
  as_tibble()|>
  pivot_longer(cols = -c("iter","chain"), names_to = "parameter", values_to = "sample")

for(country in 1:length(countries2use)){
  stacked = stacked|> mutate(
    par = str_replace_all(parameter,
                          pattern = paste0("sigma_proc\\[",country),
                          replacement = paste0("sigma_proc\\[",countries2use[country])))
}
stacked |>   
  ggplot(aes(x = iter,y = sample, colour = as.factor(chain)))+
  geom_line(alpha = .3)+
  facet_wrap(~parameter)





# do some light thinning to make the density plots faster
stacked = combo_fittedsigma_proc_mat[seq(1,nrow(combo_fittedsigma_proc_mat),by = 5),] |> 
  as_tibble()|>
  pivot_longer(cols = -c("iter","chain"), names_to = "parameter", values_to = "sample")




for(country in 1:length(countries2use)){
  stacked = stacked|> mutate(
    par = str_replace_all(parameter,
                          pattern = paste0("sigma_proc\\[",country),
                          replacement = paste0("sigma_proc\\[",countries2use[country])))
}

stacked|>   ggplot(aes(x = sample, colour = as.factor(chain)))+
  geom_density()+
  facet_wrap(~parameter, scales = "free")




```

:::




::: {.callout-note collapse=true title="process noise matrix"}
Elements of the correlation matrix from the Cholesky factorization that is used in stan:


```{r}

# Create a list of mcmc objects, one for each chain
mcmc_chains_list <- lapply(1:dim(fittedLcorr)[2], function(i) {
  # Extract the i-th chain (across the second dimension)
  # and convert it to an mcmc object
  mcmc(fittedLcorr[-c(1:burnin), i,]) 
})
# Combine the list of mcmc objects into an mcmc.list object
mclist <- as.mcmc.list(mcmc_chains_list)
gelman.diag(mclist)




# do some heavy thinning to make the traceplots plots faster
combo_fittedLcorr_mat[seq(1,nrow(combo_fittedsigma_proc_mat),by = 100),] |> 
  as_tibble()|>
  pivot_longer(cols = -c("iter","chain"), names_to = "parameter", values_to = "sample") |>   
  ggplot(aes(x = iter,y = sample, colour = as.factor(chain)))+
  geom_line(alpha = .3)+
  geom_hline(yintercept = 0)+
  facet_wrap(~parameter)





# do some light thinning to make the density plots faster
combo_fittedLcorr_mat[seq(1,nrow(combo_fittedsigma_proc_mat),by = 5),] |> 
  as_tibble()|>
  pivot_longer(cols = -c("iter","chain"), names_to = "parameter", values_to = "sample") |>   
  ggplot(aes(x = sample, colour = as.factor(chain)))+
  geom_density()+
  geom_vline(xintercept = 0)+
  facet_wrap(~parameter, scales = "free")
  
  
```




Take a closer look at the correlation structure of the process noise.  These are the lements of $\Sigma_\delta$ in the process evolution:

$$X_{t}= A+\Theta X_{t-1}+ \delta_{t}, \ \ \ where\ \ \ \delta_{t}\sim MVN(0,\Sigma_{\delta}).$$

```{r}
#| cache: true
#| code-fold: true


##### Just go right after L_proc.  It's lower triangle since it's used as 
# a cholesky decomp of the full covariance (and correlation) in the normal transitions


meanies <- lapply(1:dim(fittedLproc)[3], function(i) {
  # Extract the i-th chain (across the second dimension)
  # and convert it to an mcmc object
  mean(fittedLproc[-c(1:burnin), , i]) 
})
meanies = unlist(meanies)
names(meanies) = parnames[L_proc_index]

L_proc_mean_mat = matrix(0, nrow = length(countries2use), ncol =  length(countries2use))
for(rows in 1:length(countries2use)){
  for(cols in 1:(rows)){
    L_proc_mean_mat[rows,cols] = meanies[paste0("L_proc[",rows,",",cols,"]")]
  }
}
# reconstruct by undoing the cholesky decomp: https://mc-stan.org/docs/functions-reference/distributions_over_unbounded_vectors.html#multi-normal-cholesky-fun
Xcovmat = L_proc_mean_mat%*% t(L_proc_mean_mat)

colnames(Xcovmat) = countries2use
rownames(Xcovmat) = countries2use
df <- melt(Xcovmat)
colnames(df) <- c("Row", "Column", "Value")
ggplot(df, aes(x = Column, y = Row, fill = Value)) +
  geom_tile() +
  scale_fill_gradient(name = "Value", low = "white", high = "blue") +
  coord_fixed() +      # Keep square tiles
  labs(title = "Mean Process Covariance between countries")+
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))


diags = 1/sqrt(diag(Xcovmat))
cormat = diag(diags)%*%Xcovmat%*% diag(diags)


colnames(cormat) = countries2use
rownames(cormat) = countries2use
df <- melt(cormat)
colnames(df) <- c("Row", "Column", "Value")
ggplot(df, aes(x = Column, y = Row, fill = Value)) +
  geom_tile() +
  scale_fill_gradient(name = "Value", low = "white", high = "blue") +
  coord_fixed() +      # Keep square tiles
  labs(title = "Mean Process Correlation between countries")+
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))

```





Take a closer look at the relationship between countries.  These are the lements of $\Theta$ in the process evolution:

$$X_{t}= A+\Theta X_{t-1}+ \delta_{t}, \ \ \ where\ \ \ \delta_{t}\sim MVN(0,\Sigma_{\delta}).$$

```{r}
#| cache: true
#| code-fold: true
meanies <- lapply(1:dim(fittedtheta)[3], function(i) {
  # Extract the i-th chain (across the second dimension)
  # and convert it to an mcmc object
  mean(fittedtheta[-c(1:burnin), , i]) 
})
meanies = unlist(meanies)
names(meanies) = parnames[theta_index]





Tmean_mat = matrix(0, nrow = length(countries2use), ncol =  length(countries2use))
for(rows in 1:length(countries2use)){
  for(cols in 1:length(countries2use)){
    Tmean_mat[rows,cols] = meanies[paste0("theta[",rows,",",cols,"]")]
  }
}


colnames(Tmean_mat) = countries2use
rownames(Tmean_mat) = countries2use
df <- melt(Tmean_mat)
colnames(df) <- c("Impact_on", "Lag1", "Value")
ggplot(df, aes(x = Lag1, y = Impact_on, fill = Value)) +
  geom_tile() +
  scale_fill_gradient(name = "Value", low = "white", high = "blue") +
  coord_fixed() +      # Keep square tiles
  labs(title = "Mean Process Impact (Theta)")+
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))


```




:::

::: {.callout-note collapse=true title="Posterior distribution of states"}


```{r}
#| cache: true
#| code-fold: true


Nsam = 1000
Xsamples = combo_fittedX_mat[sample(1:nrow(combo_fittedX_mat), Nsam),]
cols <- adjustcolor(hcl.colors(Nsam, palette = "viridis"), alpha.f = .1)
for(country in 1:length(countries2use)){
  matplot(x = dates_in_use,
          y = t(Xsamples[,grep(colnames(Xsamples),
                               pattern = paste0(country,"\\]"))]),
          type = 'l',  
          main = paste("posterior predictive fit for country",countries2use[country]),
          ylim = c(-.1,.5), ylab = "Topic prevalence", col=cols)
  points(x = dates_in_use[data[,country]!=0],
         data[data[,country]!=0,country], 
         pch = "*", cex = 3, col= "red")
}

```


:::

```{r, cache = TRUE}
#| echo: false
topic_index_for_sections = topic_index_for_sections+1
topic_of_interest = candidate_topics[topic_index_for_sections]
candidate_files_to_load = list.files(pattern = paste0("stan_2chain_dataSS_",topic_of_interest,"_results_Canada_France_Germany_Italy_Japan_United Kingdom_United States_60000"))
```

## Topic `r topic_of_interest`


Results loop over the topics of interest and rely on having run the above code set across different topics.

```{r chunk784}
#| code-fold: true
runtime = filesloaded = iters_run  =  rep(NA, length(candidate_files_to_load))

# clunky and lazy way to get at the number of time steps for this topic
filename = candidate_files_to_load[1]
load(filename)
parnames = names(fit)

# collect the names of parameters

A_index             = parnames  |> grep(pattern = "^A")
theta_index         = parnames  |> grep(pattern = "theta")
sigma_obs_index     = parnames  |> grep(pattern = "sigma_obs")   
sigma_proc_index    = parnames  |> grep(pattern = "sigma_proc")
# Note that L_corr is the lower triangle cholesky decomposition of the correlation matrix so that the diagonals and upper triangle elements will be all zeros and are not sampled.
L_corr_index = NULL
for(rows in 2:length(A_index)){
  for(cols in 1:(rows-1)){
    L_corr_index = c(L_corr_index,
                     grep(parnames , pattern = paste0("L_corr\\[",rows,",",cols)))
  }
}
L_proc_index        = parnames  |> grep(pattern = "L_proc")
lp__index           = parnames  |> grep(pattern = "lp__")
X_index             = parnames  |> grep(pattern = "^X")

# collect the sampled parameters
# dimensions are: iterations, chains, parameters
fittedA          = array(NA, dim = c(niter,length(candidate_files_to_load),length(A_index)))
fittedtheta      = array(NA, dim = c(niter,length(candidate_files_to_load),length(theta_index)))
fittedsigma_obs  = array(NA, dim = c(niter,length(candidate_files_to_load),length(sigma_obs_index)))
fittedsigma_proc = array(NA, dim = c(niter,length(candidate_files_to_load),length(sigma_proc_index)))
fittedLcorr      = array(NA, dim = c(niter,length(candidate_files_to_load),length(L_corr_index)))
fittedLproc      = array(NA, dim = c(niter,length(candidate_files_to_load),length(L_proc_index)))
fittedlp__       = array(NA, dim = c(niter,length(candidate_files_to_load),length(lp__index)))
fittedX          = array(NA, dim = c(niter,length(candidate_files_to_load),length(X_index)))


for(lp in 1:length(candidate_files_to_load)){
  filename = candidate_files_to_load[lp]
  load(filename)
  runtime[lp]      = as.numeric(elapsed, units = "hours")
  filesloaded[lp]  = filename 
  iters_run[lp]    = niter
  
  fittedA[,lp,]          = extract(fit,permuted = FALSE, inc_warmup = TRUE, pars = parnames[A_index])
  fittedtheta[,lp,]      = extract(fit,permuted = FALSE, inc_warmup = TRUE, pars = parnames[theta_index])
  fittedsigma_obs[,lp,]  = extract(fit,permuted = FALSE, inc_warmup = TRUE, pars = parnames[sigma_obs_index])
  fittedsigma_proc[,lp,] = extract(fit,permuted = FALSE, inc_warmup = TRUE, pars = parnames[sigma_proc_index])
  fittedLcorr[,lp,]      = extract(fit,permuted = FALSE, inc_warmup = TRUE, pars = parnames[L_corr_index])
  fittedLproc[,lp,]      = extract(fit,permuted = FALSE, inc_warmup = TRUE, pars = parnames[L_proc_index])
  fittedlp__[,lp,]       = extract(fit,permuted = FALSE, inc_warmup = TRUE, pars = parnames[lp__index])
  fittedX[,lp,]          = extract(fit,permuted = FALSE, inc_warmup = TRUE, pars = parnames[X_index])
  
}

```

Diagnostics:

Total compute time: `r round(runtime,2)` hours.

As a sanity check, just make sure that all of the median log posteriors  are pretty close to each other.  If a subset are more than like 2% worse than others, consdier if they are sampling from different places and/or should be discarded.

```{r}
#| cache: true

niter = 60000
apply(fittedlp__[-c(1:burnin),,],2,median)

```



Consider the Gelman-Rubin Diagnostics to see if the within chain variability is similar to the between chain variability.

Make a mega matrix, but also consider little pieces later for diagnostics:

```{r}
#| code-fold: true

# reconstruct the mattrices to be easier to plot
# then slap on some names
combo_fittedA_mat           = cbind(matrix(aperm(fittedA[-c(1:burnin), , ],            c(1, 2, 3)),nrow = dim(fittedA)[1]*dim(fittedA)[2],                   ncol = dim(fittedA)[3]),          iter = rep((1+burnin):niter, dim(fittedA)[2]),         chain = rep(1:dim(fittedA)[2],           each = burnin)); colnames(combo_fittedA_mat)            = c(parnames[A_index], "iter",  "chain")
combo_fittedtheta_mat       = cbind(matrix(aperm(fittedtheta[-c(1:burnin), , ],        c(1, 2, 3)),nrow = dim(fittedtheta)[1]*dim(fittedtheta)[2],           ncol = dim(fittedtheta)[3]),      iter = rep((1+burnin):niter, dim(fittedtheta)[2]),     chain = rep(1:dim(fittedtheta)[2],       each = burnin)); colnames(combo_fittedtheta_mat)            = c(parnames[theta_index], "iter",  "chain")
combo_fittedsigma_obs_mat   = cbind(matrix(aperm(fittedsigma_obs[-c(1:burnin), , ],    c(1, 2   )),nrow = dim(fittedsigma_obs)[1]*dim(fittedsigma_obs)[2],   ncol = dim(fittedsigma_obs)[3]),  iter = rep((1+burnin):niter, dim(fittedsigma_obs)[2]), chain = rep(1:dim(fittedsigma_obs)[2],   each = burnin)); colnames(combo_fittedsigma_obs_mat)            = c(parnames[sigma_obs_index], "iter",  "chain")
combo_fittedsigma_proc_mat  = cbind(matrix(aperm(fittedsigma_proc[-c(1:burnin), , ],   c(1, 2, 3)),nrow = dim(fittedsigma_proc)[1]*dim(fittedsigma_proc)[2], ncol = dim(fittedsigma_proc)[3]), iter = rep((1+burnin):niter, dim(fittedsigma_proc)[2]),chain = rep(1:dim(fittedsigma_proc)[2],  each = burnin)); colnames(combo_fittedsigma_proc_mat)            = c(parnames[sigma_proc_index], "iter",  "chain")
combo_fittedLcorr_mat       = cbind(matrix(aperm(fittedLcorr[-c(1:burnin), , ],        c(1, 2, 3)),nrow = dim(fittedLcorr)[1]*dim(fittedLcorr)[2],           ncol = dim(fittedLcorr)[3]),      iter = rep((1+burnin):niter, dim(fittedLcorr)[2]),     chain = rep(1:dim(fittedLcorr)[2],       each = burnin)); colnames(combo_fittedLcorr_mat)            = c(parnames[L_corr_index], "iter",  "chain")
combo_fittedLproc_mat       = cbind(matrix(aperm(fittedLproc[-c(1:burnin), , ],        c(1, 2, 3)),nrow = dim(fittedLproc)[1]*dim(fittedLproc)[2],           ncol = dim(fittedLproc)[3]),      iter = rep((1+burnin):niter, dim(fittedLproc)[2]),     chain = rep(1:dim(fittedLproc)[2],       each = burnin)); colnames(combo_fittedLproc_mat)            = c(parnames[L_proc_index], "iter",  "chain")
combo_fittedlp___mat        = cbind(matrix(aperm(fittedlp__[-c(1:burnin), , ],         c(1, 2   )),nrow = dim(fittedlp__)[1]*dim(fittedlp__)[2],             ncol = dim(fittedlp__)[3]),       iter = rep((1+burnin):niter, dim(fittedlp__)[2]),      chain = rep(1:dim(fittedlp__)[2],        each = burnin)); colnames(combo_fittedlp___mat)            = c(parnames[lp__index], "iter",  "chain")
combo_fittedX_mat           = cbind(matrix(aperm(fittedX[-c(1:burnin), , ],            c(1, 2, 3)),nrow = dim(fittedX)[1]*dim(fittedX)[2],                   ncol = dim(fittedX)[3]),          iter = rep((1+burnin):niter, dim(fittedX)[2]),         chain = rep(1:dim(fittedX)[2],           each = burnin)); colnames(combo_fittedX_mat)            = c(parnames[X_index], "iter",  "chain")
```




::: {.callout-note collapse=true title="log posterior trace"}
```{r}
# do some thinning to make the traceplots plots faster
combo_fittedlp___mat[seq(1,nrow(combo_fittedlp___mat),by = 10),] |> 
  as_tibble()|>
  pivot_longer(cols = -c("iter","chain"), names_to = "parameter", values_to = "sample")|>
  ggplot(aes(x = iter,y = sample, colour = as.factor(chain)))+
  geom_line(alpha = .8)


```

:::


::: {.callout-note collapse=true title="log posterior trace"}
```{r}
# do some thinning to make the traceplots plots faster
combo_fittedlp___mat[seq(1,nrow(combo_fittedlp___mat),by = 10),] |> 
  as_tibble()|>
  pivot_longer(cols = -c("iter","chain"), names_to = "parameter", values_to = "sample")|>
  ggplot(aes(x = iter,y = sample, colour = as.factor(chain)))+
  geom_line(alpha = .8)


```

:::

::: {.callout-note collapse=true title="A"}






```{r}

# Create a list of mcmc objects, one for each chain
mcmc_chains_list <- lapply(1:dim(fittedA)[2], function(i) {
  # Extract the i-th chain (across the second dimension)
  # and convert it to an mcmc object
  mcmc(fittedA[-c(1:burnin),i,])
})
# Combine the list of mcmc objects into an mcmc.list object
mclist <- as.mcmc.list(mcmc_chains_list)
gelman.diag(mclist)

# do some heavy thinning to make the traceplots plots faster
stacked = combo_fittedA_mat[seq(1,nrow(combo_fittedA_mat), by = 50),] |>
  as_tibble()|>
  pivot_longer(cols = -c("iter","chain"), names_to = "parameter", values_to = "sample")

for(country in 1:length(countries2use)){
  stacked = stacked|> mutate(
    parameter = str_replace_all(parameter,
                                pattern = paste0("A\\[",country),
                                replacement = paste0("A\\[",countries2use[country])))
}
stacked |>   ggplot(aes(x = iter,y = sample, colour = as.factor(chain)))+
  geom_line(alpha = .3)+
  facet_wrap(~parameter)






# do some light thinning to make the density plots faster
stacked = combo_fittedA_mat[seq(1,nrow(combo_fittedA_mat), by = 5),] |>
  as_tibble()|>
  pivot_longer(cols = -c("iter","chain"), names_to = "parameter", values_to = "sample")


for(country in 1:length(countries2use)){
  stacked = stacked|> mutate(
    parameter = str_replace_all(parameter,
                                pattern = paste0("A\\[",country),
                                replacement = paste0("A\\[",countries2use[country])))
}

stacked|>   ggplot(aes(x = sample, colour = as.factor(chain)))+
  geom_density()+
  facet_wrap(~parameter, scales = "free")


```
:::

::: {.callout-note collapse=true title="theta"}

```{r}


# Create a list of mcmc objects, one for each chain
mcmc_chains_list <- lapply(1:dim(fittedtheta)[2], function(i) {
  # Extract the i-th chain (across the second dimension)
  # and convert it to an mcmc object
  mcmc(fittedtheta[-c(1:burnin),i,])
})
# Combine the list of mcmc objects into an mcmc.list object
mclist <- as.mcmc.list(mcmc_chains_list)

gelman.diag(mclist)


# do some heavy thinning to make the traceplots plots faster
stacked = combo_fittedtheta_mat[seq(1,nrow(combo_fittedtheta_mat), by = 100),] |>  
  as_tibble()|>
  pivot_longer(cols = -c("iter","chain"), names_to = "parameter", values_to = "sample")



stacked |>   ggplot(aes(x = iter,y = sample, colour = as.factor(chain)))+
  geom_line(alpha = .3)+
  facet_wrap(~parameter)


# do some light thinning to make the density plots faster
combo_fittedtheta_mat[seq(1,nrow(combo_fittedtheta_mat), by = 5),] |>  
  as_tibble()|>
  pivot_longer(cols = -c("iter","chain"), names_to = "parameter", values_to = "sample")|>
  ggplot(aes(x = sample, colour = as.factor(chain)))+
  geom_density()+
  geom_vline(xintercept = 0)+
  facet_wrap(~parameter, scales = "free_y")


```

:::


::: {.callout-note collapse=true title="observational noise"}

The $\sigma_{\e}$ aka sigma\_obs, the observational noise:



```{r}

# Create a list of mcmc objects, one for each chain
mcmc_chains_list <- lapply(1:dim(fittedsigma_obs)[2], function(i) {
  # Extract the i-th chain (across the second dimension)
  # and convert it to an mcmc object
  mcmc(fittedsigma_obs[-c(1:burnin), i,]) 
})
# Combine the list of mcmc objects into an mcmc.list object
mclist <- as.mcmc.list(mcmc_chains_list)
gelman.diag(mclist)




# do some heavy thinning to make the traceplots plots faster
combo_fittedsigma_obs_mat[seq(1,nrow(combo_fittedsigma_obs_mat),by = 100),] |> 
  as_tibble()|>
  pivot_longer(cols = -c("iter","chain"), names_to = "parameter", values_to = "sample")|>
  ggplot(aes(x = iter,y = sample, colour = as.factor(chain)))+
  geom_line(alpha = .3)





# do some light thinning to make the density plots faster
combo_fittedsigma_obs_mat[seq(1,nrow(combo_fittedsigma_obs_mat),by = 5),] |> 
  as_tibble()|>
  pivot_longer(cols = -c("iter","chain"), names_to = "parameter", values_to = "sample")|>
  ggplot(aes(x = sample, colour = as.factor(chain)))+
  geom_density()



```
:::



::: {.callout-note collapse=true title="process noise"}

The $\sigma_{\delta}$ aka sigma\_proc.  These are the diagonal elements of the process noise.

```{r chunk1033}

# Create a list of mcmc objects, one for each chain
mcmc_chains_list <- lapply(1:dim(fittedsigma_proc)[2], function(i) {
  # Extract the i-th chain (across the second dimension)
  # and convert it to an mcmc object
  mcmc(fittedsigma_proc[-c(1:burnin), i,]) 
})
# Combine the list of mcmc objects into an mcmc.list object
mclist <- as.mcmc.list(mcmc_chains_list)
gelman.diag(mclist)




# do some heavy thinning to make the traceplots plots faster
stacked = combo_fittedsigma_proc_mat[seq(1,nrow(combo_fittedsigma_proc_mat),by = 100),] |> 
  as_tibble()|>
  pivot_longer(cols = -c("iter","chain"), names_to = "parameter", values_to = "sample")

for(country in 1:length(countries2use)){
  stacked = stacked|> mutate(
    par = str_replace_all(parameter,
                          pattern = paste0("sigma_proc\\[",country),
                          replacement = paste0("sigma_proc\\[",countries2use[country])))
}
stacked |>   
  ggplot(aes(x = iter,y = sample, colour = as.factor(chain)))+
  geom_line(alpha = .3)+
  facet_wrap(~parameter)





# do some light thinning to make the density plots faster
stacked = combo_fittedsigma_proc_mat[seq(1,nrow(combo_fittedsigma_proc_mat),by = 5),] |> 
  as_tibble()|>
  pivot_longer(cols = -c("iter","chain"), names_to = "parameter", values_to = "sample")




for(country in 1:length(countries2use)){
  stacked = stacked|> mutate(
    par = str_replace_all(parameter,
                          pattern = paste0("sigma_proc\\[",country),
                          replacement = paste0("sigma_proc\\[",countries2use[country])))
}

stacked|>   ggplot(aes(x = sample, colour = as.factor(chain)))+
  geom_density()+
  facet_wrap(~parameter, scales = "free")




```

:::




::: {.callout-note collapse=true title="process noise matrix"}
Elements of the correlation matrix from the Cholesky factorization that is used in stan:


```{r}

# Create a list of mcmc objects, one for each chain
mcmc_chains_list <- lapply(1:dim(fittedLcorr)[2], function(i) {
  # Extract the i-th chain (across the second dimension)
  # and convert it to an mcmc object
  mcmc(fittedLcorr[-c(1:burnin), i,]) 
})
# Combine the list of mcmc objects into an mcmc.list object
mclist <- as.mcmc.list(mcmc_chains_list)
gelman.diag(mclist)




# do some heavy thinning to make the traceplots plots faster
combo_fittedLcorr_mat[seq(1,nrow(combo_fittedsigma_proc_mat),by = 100),] |> 
  as_tibble()|>
  pivot_longer(cols = -c("iter","chain"), names_to = "parameter", values_to = "sample") |>   
  ggplot(aes(x = iter,y = sample, colour = as.factor(chain)))+
  geom_line(alpha = .3)+
  geom_hline(yintercept = 0)+
  facet_wrap(~parameter)





# do some light thinning to make the density plots faster
combo_fittedLcorr_mat[seq(1,nrow(combo_fittedsigma_proc_mat),by = 5),] |> 
  as_tibble()|>
  pivot_longer(cols = -c("iter","chain"), names_to = "parameter", values_to = "sample") |>   
  ggplot(aes(x = sample, colour = as.factor(chain)))+
  geom_density()+
  geom_vline(xintercept = 0)+
  facet_wrap(~parameter, scales = "free")
  
  

```




Take a closer look at the correlation structure of the process noise.  These are the lements of $\Sigma_\delta$ in the process evolution:

$$X_{t}= A+\Theta X_{t-1}+ \delta_{t}, \ \ \ where\ \ \ \delta_{t}\sim MVN(0,\Sigma_{\delta}).$$

```{r}
#| cache: true
#| code-fold: true


##### Just go right after L_proc.  It's lower triangle since it's used as 
# a cholesky decomp of the full covariance (and correlation) in the normal transitions



meanies <- lapply(1:dim(fittedLproc)[3], function(i) {
  # Extract the i-th chain (across the second dimension)
  # and convert it to an mcmc object
  mean(fittedLproc[-c(1:burnin), , i]) 
})
meanies = unlist(meanies)
names(meanies) = parnames[L_proc_index]

L_proc_mean_mat = matrix(0, nrow = length(countries2use), ncol =  length(countries2use))
for(rows in 1:length(countries2use)){
  for(cols in 1:(rows)){
    L_proc_mean_mat[rows,cols] = meanies[paste0("L_proc[",rows,",",cols,"]")]
  }
}
# reconstruct by undoing the cholesky decomp: https://mc-stan.org/docs/functions-reference/distributions_over_unbounded_vectors.html#multi-normal-cholesky-fun
Xcovmat = L_proc_mean_mat%*% t(L_proc_mean_mat)

colnames(Xcovmat) = countries2use
rownames(Xcovmat) = countries2use
df <- melt(Xcovmat)
colnames(df) <- c("Row", "Column", "Value")
ggplot(df, aes(x = Column, y = Row, fill = Value)) +
  geom_tile() +
  scale_fill_gradient(name = "Value", low = "white", high = "blue") +
  coord_fixed() +      # Keep square tiles
  labs(title = "Mean Process Covariance between countries")+
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))


diags = 1/sqrt(diag(Xcovmat))
cormat = diag(diags)%*%Xcovmat%*% diag(diags)


colnames(cormat) = countries2use
rownames(cormat) = countries2use
df <- melt(cormat)
colnames(df) <- c("Row", "Column", "Value")
ggplot(df, aes(x = Column, y = Row, fill = Value)) +
  geom_tile() +
  scale_fill_gradient(name = "Value", low = "white", high = "blue") +
  coord_fixed() +      # Keep square tiles
  labs(title = "Mean Process Correlation between countries")+
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))

```





Take a closer look at the relationship between countries.  These are the lements of $\Theta$ in the process evolution:

$$X_{t}= A+\Theta X_{t-1}+ \delta_{t}, \ \ \ where\ \ \ \delta_{t}\sim MVN(0,\Sigma_{\delta}).$$

```{r}
#| cache: true
#| code-fold: true



meanies <- lapply(1:dim(fittedtheta)[3], function(i) {
  # Extract the i-th chain (across the second dimension)
  # and convert it to an mcmc object
  mean(fittedtheta[-c(1:burnin), , i]) 
})
meanies = unlist(meanies)
names(meanies) = parnames[theta_index]





Tmean_mat = matrix(0, nrow = length(countries2use), ncol =  length(countries2use))
for(rows in 1:length(countries2use)){
  for(cols in 1:length(countries2use)){
    Tmean_mat[rows,cols] = meanies[paste0("theta[",rows,",",cols,"]")]
  }
}


colnames(Tmean_mat) = countries2use
rownames(Tmean_mat) = countries2use
df <- melt(Tmean_mat)
colnames(df) <- c("Impact_on", "Lag1", "Value")
ggplot(df, aes(x = Lag1, y = Impact_on, fill = Value)) +
  geom_tile() +
  scale_fill_gradient(name = "Value", low = "white", high = "blue") +
  coord_fixed() +      # Keep square tiles
  labs(title = "Mean Process Impact (Theta)")+
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))


```




:::


::: {.callout-note collapse=true title="Posterior distribution of states"}


```{r}
#| cache: true
#| code-fold: true

Nsam = 1000
Xsamples = combo_fittedX_mat[sample(1:nrow(combo_fittedX_mat), Nsam),]
cols <- adjustcolor(hcl.colors(Nsam, palette = "viridis"), alpha.f = .1)
for(country in 1:length(countries2use)){
  matplot(x = dates_in_use,
          y = t(Xsamples[,grep(colnames(Xsamples),
                               pattern = paste0(country,"\\]"))]),
          type = 'l',  
          main = paste("posterior predictive fit for country",countries2use[country]),
          ylim = c(-.1,.5), ylab = "Topic prevalence", col=cols)
  points(x = dates_in_use[data[,country]!=0],
         data[data[,country]!=0,country], 
         pch = "*", cex = 3, col= "red")
}

```





:::



```{r, cache = TRUE}
#| echo: false
topic_index_for_sections = topic_index_for_sections+1
topic_of_interest = candidate_topics[topic_index_for_sections]
candidate_files_to_load = list.files(pattern = paste0("stan_2chain_dataSS_",topic_of_interest,"_results_Canada_France_Germany_Italy_Japan_United Kingdom_United States_60000"))
```

## Topic `r topic_of_interest`


Results loop over the topics of interest and rely on having run the above code set across different topics.

```{r chunk1303}
#| code-fold: true
runtime = filesloaded = iters_run  =  rep(NA, length(candidate_files_to_load))

# clunky and lazy way to get at the number of time steps for this topic
filename = candidate_files_to_load[1]
load(filename)
parnames = names(fit)

# collect the names of parameters

A_index             = parnames  |> grep(pattern = "^A")
theta_index         = parnames  |> grep(pattern = "theta")
sigma_obs_index     = parnames  |> grep(pattern = "sigma_obs")   
sigma_proc_index    = parnames  |> grep(pattern = "sigma_proc")
# Note that L_corr is the lower triangle cholesky decomposition of the correlation matrix so that the diagonals and upper triangle elements will be all zeros and are not sampled.
L_corr_index = NULL
for(rows in 2:length(A_index)){
  for(cols in 1:(rows-1)){
    L_corr_index = c(L_corr_index,
                     grep(parnames , pattern = paste0("L_corr\\[",rows,",",cols)))
  }
}
L_proc_index        = parnames  |> grep(pattern = "L_proc")
lp__index           = parnames  |> grep(pattern = "lp__")
X_index             = parnames  |> grep(pattern = "^X")

# collect the sampled parameters
# dimensions are: iterations, chains, parameters
fittedA          = array(NA, dim = c(niter,length(candidate_files_to_load),length(A_index)))
fittedtheta      = array(NA, dim = c(niter,length(candidate_files_to_load),length(theta_index)))
fittedsigma_obs  = array(NA, dim = c(niter,length(candidate_files_to_load),length(sigma_obs_index)))
fittedsigma_proc = array(NA, dim = c(niter,length(candidate_files_to_load),length(sigma_proc_index)))
fittedLcorr      = array(NA, dim = c(niter,length(candidate_files_to_load),length(L_corr_index)))
fittedLproc      = array(NA, dim = c(niter,length(candidate_files_to_load),length(L_proc_index)))
fittedlp__       = array(NA, dim = c(niter,length(candidate_files_to_load),length(lp__index)))
fittedX          = array(NA, dim = c(niter,length(candidate_files_to_load),length(X_index)))


for(lp in 1:length(candidate_files_to_load)){
  filename = candidate_files_to_load[lp]
  load(filename)
  runtime[lp]      = as.numeric(elapsed, units = "hours")
  filesloaded[lp]  = filename 
  iters_run[lp]    = niter
  
  fittedA[,lp,]          = extract(fit,permuted = FALSE, inc_warmup = TRUE, pars = parnames[A_index])
  fittedtheta[,lp,]      = extract(fit,permuted = FALSE, inc_warmup = TRUE, pars = parnames[theta_index])
  fittedsigma_obs[,lp,]  = extract(fit,permuted = FALSE, inc_warmup = TRUE, pars = parnames[sigma_obs_index])
  fittedsigma_proc[,lp,] = extract(fit,permuted = FALSE, inc_warmup = TRUE, pars = parnames[sigma_proc_index])
  fittedLcorr[,lp,]      = extract(fit,permuted = FALSE, inc_warmup = TRUE, pars = parnames[L_corr_index])
  fittedLproc[,lp,]      = extract(fit,permuted = FALSE, inc_warmup = TRUE, pars = parnames[L_proc_index])
  fittedlp__[,lp,]       = extract(fit,permuted = FALSE, inc_warmup = TRUE, pars = parnames[lp__index])
  fittedX[,lp,]          = extract(fit,permuted = FALSE, inc_warmup = TRUE, pars = parnames[X_index])
  
}

```

Diagnostics:

Total compute time: `r round(runtime,2)` hours.

As a sanity check, just make sure that all of the median log posteriors  are pretty close to each other.  If a subset are more than like 2% worse than others, consdier if they are sampling from different places and/or should be discarded.

```{r}
#| cache: true

niter = 60000
apply(fittedlp__[-c(1:burnin),,],2,median)

```



Consider the Gelman-Rubin Diagnostics to see if the within chain variability is similar to the between chain variability.

Make a mega matrix, but also consider little pieces later for diagnostics:

```{r}
#| code-fold: true


# reconstruct the mattrices to be easier to plot
# then slap on some names
combo_fittedA_mat           = cbind(matrix(aperm(fittedA[-c(1:burnin), , ],            c(1, 2, 3)),nrow = dim(fittedA)[1]*dim(fittedA)[2],                   ncol = dim(fittedA)[3]),          iter = rep((1+burnin):niter, dim(fittedA)[2]),         chain = rep(1:dim(fittedA)[2],           each = burnin)); colnames(combo_fittedA_mat)            = c(parnames[A_index], "iter",  "chain")
combo_fittedtheta_mat       = cbind(matrix(aperm(fittedtheta[-c(1:burnin), , ],        c(1, 2, 3)),nrow = dim(fittedtheta)[1]*dim(fittedtheta)[2],           ncol = dim(fittedtheta)[3]),      iter = rep((1+burnin):niter, dim(fittedtheta)[2]),     chain = rep(1:dim(fittedtheta)[2],       each = burnin)); colnames(combo_fittedtheta_mat)            = c(parnames[theta_index], "iter",  "chain")
combo_fittedsigma_obs_mat   = cbind(matrix(aperm(fittedsigma_obs[-c(1:burnin), , ],    c(1, 2   )),nrow = dim(fittedsigma_obs)[1]*dim(fittedsigma_obs)[2],   ncol = dim(fittedsigma_obs)[3]),  iter = rep((1+burnin):niter, dim(fittedsigma_obs)[2]), chain = rep(1:dim(fittedsigma_obs)[2],   each = burnin)); colnames(combo_fittedsigma_obs_mat)            = c(parnames[sigma_obs_index], "iter",  "chain")
combo_fittedsigma_proc_mat  = cbind(matrix(aperm(fittedsigma_proc[-c(1:burnin), , ],   c(1, 2, 3)),nrow = dim(fittedsigma_proc)[1]*dim(fittedsigma_proc)[2], ncol = dim(fittedsigma_proc)[3]), iter = rep((1+burnin):niter, dim(fittedsigma_proc)[2]),chain = rep(1:dim(fittedsigma_proc)[2],  each = burnin)); colnames(combo_fittedsigma_proc_mat)            = c(parnames[sigma_proc_index], "iter",  "chain")
combo_fittedLcorr_mat       = cbind(matrix(aperm(fittedLcorr[-c(1:burnin), , ],        c(1, 2, 3)),nrow = dim(fittedLcorr)[1]*dim(fittedLcorr)[2],           ncol = dim(fittedLcorr)[3]),      iter = rep((1+burnin):niter, dim(fittedLcorr)[2]),     chain = rep(1:dim(fittedLcorr)[2],       each = burnin)); colnames(combo_fittedLcorr_mat)            = c(parnames[L_corr_index], "iter",  "chain")
combo_fittedLproc_mat       = cbind(matrix(aperm(fittedLproc[-c(1:burnin), , ],        c(1, 2, 3)),nrow = dim(fittedLproc)[1]*dim(fittedLproc)[2],           ncol = dim(fittedLproc)[3]),      iter = rep((1+burnin):niter, dim(fittedLproc)[2]),     chain = rep(1:dim(fittedLproc)[2],       each = burnin)); colnames(combo_fittedLproc_mat)            = c(parnames[L_proc_index], "iter",  "chain")
combo_fittedlp___mat        = cbind(matrix(aperm(fittedlp__[-c(1:burnin), , ],         c(1, 2   )),nrow = dim(fittedlp__)[1]*dim(fittedlp__)[2],             ncol = dim(fittedlp__)[3]),       iter = rep((1+burnin):niter, dim(fittedlp__)[2]),      chain = rep(1:dim(fittedlp__)[2],        each = burnin)); colnames(combo_fittedlp___mat)            = c(parnames[lp__index], "iter",  "chain")
combo_fittedX_mat           = cbind(matrix(aperm(fittedX[-c(1:burnin), , ],            c(1, 2, 3)),nrow = dim(fittedX)[1]*dim(fittedX)[2],                   ncol = dim(fittedX)[3]),          iter = rep((1+burnin):niter, dim(fittedX)[2]),         chain = rep(1:dim(fittedX)[2],           each = burnin)); colnames(combo_fittedX_mat)            = c(parnames[X_index], "iter",  "chain")
```



::: {.callout-note collapse=true title="log posterior trace"}
```{r}
# do some thinning to make the traceplots plots faster
combo_fittedlp___mat[seq(1,nrow(combo_fittedlp___mat),by = 10),] |> 
  as_tibble()|>
  pivot_longer(cols = -c("iter","chain"), names_to = "parameter", values_to = "sample")|>
  ggplot(aes(x = iter,y = sample, colour = as.factor(chain)))+
  geom_line(alpha = .8)


```

:::

::: {.callout-note collapse=true title="A"}







```{r chunk1407}

# Create a list of mcmc objects, one for each chain
mcmc_chains_list <- lapply(1:dim(fittedA)[2], function(i) {
  # Extract the i-th chain (across the second dimension)
  # and convert it to an mcmc object
  mcmc(fittedA[-c(1:burnin),i,])
})
# Combine the list of mcmc objects into an mcmc.list object
mclist <- as.mcmc.list(mcmc_chains_list)
gelman.diag(mclist)

# do some heavy thinning to make the traceplots plots faster
stacked = combo_fittedA_mat[seq(1,nrow(combo_fittedA_mat), by = 50),] |>
  as_tibble()|>
  pivot_longer(cols = -c("iter","chain"), names_to = "parameter", values_to = "sample")

for(country in 1:length(countries2use)){
  stacked = stacked|> mutate(
    parameter = str_replace_all(parameter,
                                pattern = paste0("A\\[",country),
                                replacement = paste0("A\\[",countries2use[country])))
}
stacked |>   ggplot(aes(x = iter,y = sample, colour = as.factor(chain)))+
  geom_line(alpha = .3)+
  facet_wrap(~parameter)






# do some light thinning to make the density plots faster
stacked = combo_fittedA_mat[seq(1,nrow(combo_fittedA_mat), by = 5),] |>
  as_tibble()|>
  pivot_longer(cols = -c("iter","chain"), names_to = "parameter", values_to = "sample")


for(country in 1:length(countries2use)){
  stacked = stacked|> mutate(
    parameter = str_replace_all(parameter,
                                pattern = paste0("A\\[",country),
                                replacement = paste0("A\\[",countries2use[country])))
}

stacked|>   ggplot(aes(x = sample, colour = as.factor(chain)))+
  geom_density()+
  facet_wrap(~parameter, scales = "free")


```
:::

::: {.callout-note collapse=true title="theta"}

```{r}


# Create a list of mcmc objects, one for each chain
mcmc_chains_list <- lapply(1:dim(fittedtheta)[2], function(i) {
  # Extract the i-th chain (across the second dimension)
  # and convert it to an mcmc object
  mcmc(fittedtheta[-c(1:burnin),i,])
})
# Combine the list of mcmc objects into an mcmc.list object
mclist <- as.mcmc.list(mcmc_chains_list)

gelman.diag(mclist)


# do some heavy thinning to make the traceplots plots faster
stacked = combo_fittedtheta_mat[seq(1,nrow(combo_fittedtheta_mat), by = 100),] |>  
  as_tibble()|>
  pivot_longer(cols = -c("iter","chain"), names_to = "parameter", values_to = "sample")



stacked |>   ggplot(aes(x = iter,y = sample, colour = as.factor(chain)))+
  geom_line(alpha = .3)+
  facet_wrap(~parameter)


# do some light thinning to make the density plots faster
combo_fittedtheta_mat[seq(1,nrow(combo_fittedtheta_mat), by = 5),] |>  
  as_tibble()|>
  pivot_longer(cols = -c("iter","chain"), names_to = "parameter", values_to = "sample")|>
  ggplot(aes(x = sample, colour = as.factor(chain)))+
  geom_density()+
  geom_vline(xintercept = 0)+
  facet_wrap(~parameter, scales = "free_y")


```

:::


::: {.callout-note collapse=true title="observational noise"}

The $\sigma_{\e}$ aka sigma\_obs, the observational noise:



```{r}

# Create a list of mcmc objects, one for each chain
mcmc_chains_list <- lapply(1:dim(fittedsigma_obs)[2], function(i) {
  # Extract the i-th chain (across the second dimension)
  # and convert it to an mcmc object
  mcmc(fittedsigma_obs[-c(1:burnin), i,]) 
})
# Combine the list of mcmc objects into an mcmc.list object
mclist <- as.mcmc.list(mcmc_chains_list)
gelman.diag(mclist)




# do some heavy thinning to make the traceplots plots faster
combo_fittedsigma_obs_mat[seq(1,nrow(combo_fittedsigma_obs_mat),by = 100),] |> 
  as_tibble()|>
  pivot_longer(cols = -c("iter","chain"), names_to = "parameter", values_to = "sample")|>
  ggplot(aes(x = iter,y = sample, colour = as.factor(chain)))+
  geom_line(alpha = .3)





# do some light thinning to make the density plots faster
combo_fittedsigma_obs_mat[seq(1,nrow(combo_fittedsigma_obs_mat),by = 5),] |> 
  as_tibble()|>
  pivot_longer(cols = -c("iter","chain"), names_to = "parameter", values_to = "sample")|>
  ggplot(aes(x = sample, colour = as.factor(chain)))+
  geom_density()



```
:::



::: {.callout-note collapse=true title="process noise"}

The $\sigma_{\delta}$ aka sigma\_proc.  These are the diagonal elements of the process noise.

```{r}

# Create a list of mcmc objects, one for each chain
mcmc_chains_list <- lapply(1:dim(fittedsigma_proc)[2], function(i) {
  # Extract the i-th chain (across the second dimension)
  # and convert it to an mcmc object
  mcmc(fittedsigma_proc[-c(1:burnin), i,]) 
})
# Combine the list of mcmc objects into an mcmc.list object
mclist <- as.mcmc.list(mcmc_chains_list)
gelman.diag(mclist)




# do some heavy thinning to make the traceplots plots faster
stacked = combo_fittedsigma_proc_mat[seq(1,nrow(combo_fittedsigma_proc_mat),by = 100),] |> 
  as_tibble()|>
  pivot_longer(cols = -c("iter","chain"), names_to = "parameter", values_to = "sample")

for(country in 1:length(countries2use)){
  stacked = stacked|> mutate(
    par = str_replace_all(parameter,
                          pattern = paste0("sigma_proc\\[",country),
                          replacement = paste0("sigma_proc\\[",countries2use[country])))
}
stacked |>   
  ggplot(aes(x = iter,y = sample, colour = as.factor(chain)))+
  geom_line(alpha = .3)+
  facet_wrap(~parameter)





# do some light thinning to make the density plots faster
stacked = combo_fittedsigma_proc_mat[seq(1,nrow(combo_fittedsigma_proc_mat),by = 5),] |> 
  as_tibble()|>
  pivot_longer(cols = -c("iter","chain"), names_to = "parameter", values_to = "sample")




for(country in 1:length(countries2use)){
  stacked = stacked|> mutate(
    par = str_replace_all(parameter,
                          pattern = paste0("sigma_proc\\[",country),
                          replacement = paste0("sigma_proc\\[",countries2use[country])))
}

stacked|>   ggplot(aes(x = sample, colour = as.factor(chain)))+
  geom_density()+
  facet_wrap(~parameter, scales = "free")




```

:::




::: {.callout-note collapse=true title="process noise matrix"}
Elements of the correlation matrix from the Cholesky factorization that is used in stan:


```{r}

# Create a list of mcmc objects, one for each chain
mcmc_chains_list <- lapply(1:dim(fittedLcorr)[2], function(i) {
  # Extract the i-th chain (across the second dimension)
  # and convert it to an mcmc object
  mcmc(fittedLcorr[-c(1:burnin), i,]) 
})
# Combine the list of mcmc objects into an mcmc.list object
mclist <- as.mcmc.list(mcmc_chains_list)
gelman.diag(mclist)




# do some heavy thinning to make the traceplots plots faster
combo_fittedLcorr_mat[seq(1,nrow(combo_fittedsigma_proc_mat),by = 100),] |> 
  as_tibble()|>
  pivot_longer(cols = -c("iter","chain"), names_to = "parameter", values_to = "sample") |>   
  ggplot(aes(x = iter,y = sample, colour = as.factor(chain)))+
  geom_line(alpha = .3)+
  geom_hline(yintercept = 0)+
  facet_wrap(~parameter)





# do some light thinning to make the density plots faster
combo_fittedLcorr_mat[seq(1,nrow(combo_fittedsigma_proc_mat),by = 5),] |> 
  as_tibble()|>
  pivot_longer(cols = -c("iter","chain"), names_to = "parameter", values_to = "sample") |>   
  ggplot(aes(x = sample, colour = as.factor(chain)))+
  geom_density()+
  geom_vline(xintercept = 0)+
  facet_wrap(~parameter, scales = "free")
  
  

```




Take a closer look at the correlation structure of the process noise.  These are the lements of $\Sigma_\delta$ in the process evolution:

$$X_{t}= A+\Theta X_{t-1}+ \delta_{t}, \ \ \ where\ \ \ \delta_{t}\sim MVN(0,\Sigma_{\delta}).$$

```{r}
#| cache: true
#| code-fold: true


##### Just go right after L_proc.  It's lower triangle since it's used as 
# a cholesky decomp of the full covariance (and correlation) in the normal transitions



meanies <- lapply(1:dim(fittedLproc)[3], function(i) {
  # Extract the i-th chain (across the second dimension)
  # and convert it to an mcmc object
  mean(fittedLproc[-c(1:burnin), , i]) 
})
meanies = unlist(meanies)
names(meanies) = parnames[L_proc_index]

L_proc_mean_mat = matrix(0, nrow = length(countries2use), ncol =  length(countries2use))
for(rows in 1:length(countries2use)){
  for(cols in 1:(rows)){
    L_proc_mean_mat[rows,cols] = meanies[paste0("L_proc[",rows,",",cols,"]")]
  }
}
# reconstruct by undoing the cholesky decomp: https://mc-stan.org/docs/functions-reference/distributions_over_unbounded_vectors.html#multi-normal-cholesky-fun
Xcovmat = L_proc_mean_mat%*% t(L_proc_mean_mat)

colnames(Xcovmat) = countries2use
rownames(Xcovmat) = countries2use
df <- melt(Xcovmat)
colnames(df) <- c("Row", "Column", "Value")
ggplot(df, aes(x = Column, y = Row, fill = Value)) +
  geom_tile() +
  scale_fill_gradient(name = "Value", low = "white", high = "blue") +
  coord_fixed() +      # Keep square tiles
  labs(title = "Mean Process Covariance between countries")+
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))


diags = 1/sqrt(diag(Xcovmat))
cormat = diag(diags)%*%Xcovmat%*% diag(diags)


colnames(cormat) = countries2use
rownames(cormat) = countries2use
df <- melt(cormat)
colnames(df) <- c("Row", "Column", "Value")
ggplot(df, aes(x = Column, y = Row, fill = Value)) +
  geom_tile() +
  scale_fill_gradient(name = "Value", low = "white", high = "blue") +
  coord_fixed() +      # Keep square tiles
  labs(title = "Mean Process Correlation between countries")+
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))

```





Take a closer look at the relationship between countries.  These are the lements of $\Theta$ in the process evolution:

$$X_{t}= A+\Theta X_{t-1}+ \delta_{t}, \ \ \ where\ \ \ \delta_{t}\sim MVN(0,\Sigma_{\delta}).$$

```{r}
#| cache: true
#| code-fold: true



meanies <- lapply(1:dim(fittedtheta)[3], function(i) {
  # Extract the i-th chain (across the second dimension)
  # and convert it to an mcmc object
  mean(fittedtheta[-c(1:burnin), , i]) 
})
meanies = unlist(meanies)
names(meanies) = parnames[theta_index]





Tmean_mat = matrix(0, nrow = length(countries2use), ncol =  length(countries2use))
for(rows in 1:length(countries2use)){
  for(cols in 1:length(countries2use)){
    Tmean_mat[rows,cols] = meanies[paste0("theta[",rows,",",cols,"]")]
  }
}


colnames(Tmean_mat) = countries2use
rownames(Tmean_mat) = countries2use
df <- melt(Tmean_mat)
colnames(df) <- c("Impact_on", "Lag1", "Value")
ggplot(df, aes(x = Lag1, y = Impact_on, fill = Value)) +
  geom_tile() +
  scale_fill_gradient(name = "Value", low = "white", high = "blue") +
  coord_fixed() +      # Keep square tiles
  labs(title = "Mean Process Impact (Theta)")+
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))


```




:::

::: {.callout-note collapse=true title="Posterior distribution of states"}


```{r}
#| cache: true
#| code-fold: true

Nsam = 1000
Xsamples = combo_fittedX_mat[sample(1:nrow(combo_fittedX_mat), Nsam),]
cols <- adjustcolor(hcl.colors(Nsam, palette = "viridis"), alpha.f = .1)
for(country in 1:length(countries2use)){
  matplot(x = dates_in_use,
          y = t(Xsamples[,grep(colnames(Xsamples),
                               pattern = paste0(country,"\\]"))]),
          type = 'l',  
          main = paste("posterior predictive fit for country",countries2use[country]),
          ylim = c(-.1,.5), ylab = "Topic prevalence", col=cols)
  points(x = dates_in_use[data[,country]!=0],
         data[data[,country]!=0,country], 
         pch = "*", cex = 3, col= "red")
}

```



:::


```{r, cache = TRUE}
#| echo: false
topic_index_for_sections = topic_index_for_sections+1
topic_of_interest = candidate_topics[topic_index_for_sections]
candidate_files_to_load = list.files(pattern = paste0("stan_2chain_dataSS_",topic_of_interest,"_results_Canada_France_Germany_Italy_Japan_United Kingdom_United States_60000"))
```

## Topic `r topic_of_interest`


Results loop over the topics of interest and rely on having run the above code set across different topics.

```{r chunk1819}
#| code-fold: true
runtime = filesloaded = iters_run  =  rep(NA, length(candidate_files_to_load))

# clunky and lazy way to get at the number of time steps for this topic
filename = candidate_files_to_load[1]
load(filename)
parnames = names(fit)

# collect the names of parameters

A_index             = parnames  |> grep(pattern = "^A")
theta_index         = parnames  |> grep(pattern = "theta")
sigma_obs_index     = parnames  |> grep(pattern = "sigma_obs")   
sigma_proc_index    = parnames  |> grep(pattern = "sigma_proc")
# Note that L_corr is the lower triangle cholesky decomposition of the correlation matrix so that the diagonals and upper triangle elements will be all zeros and are not sampled.
L_corr_index = NULL
for(rows in 2:length(A_index)){
  for(cols in 1:(rows-1)){
    L_corr_index = c(L_corr_index,
                     grep(parnames , pattern = paste0("L_corr\\[",rows,",",cols)))
  }
}
L_proc_index        = parnames  |> grep(pattern = "L_proc")
lp__index           = parnames  |> grep(pattern = "lp__")
X_index             = parnames  |> grep(pattern = "^X")

# collect the sampled parameters
# dimensions are: iterations, chains, parameters
fittedA          = array(NA, dim = c(niter,length(candidate_files_to_load),length(A_index)))
fittedtheta      = array(NA, dim = c(niter,length(candidate_files_to_load),length(theta_index)))
fittedsigma_obs  = array(NA, dim = c(niter,length(candidate_files_to_load),length(sigma_obs_index)))
fittedsigma_proc = array(NA, dim = c(niter,length(candidate_files_to_load),length(sigma_proc_index)))
fittedLcorr      = array(NA, dim = c(niter,length(candidate_files_to_load),length(L_corr_index)))
fittedLproc      = array(NA, dim = c(niter,length(candidate_files_to_load),length(L_proc_index)))
fittedlp__       = array(NA, dim = c(niter,length(candidate_files_to_load),length(lp__index)))
fittedX          = array(NA, dim = c(niter,length(candidate_files_to_load),length(X_index)))


for(lp in 1:length(candidate_files_to_load)){
  filename = candidate_files_to_load[lp]
  load(filename)
  runtime[lp]      = as.numeric(elapsed, units = "hours")
  filesloaded[lp]  = filename 
  iters_run[lp]    = niter
  
  fittedA[,lp,]          = extract(fit,permuted = FALSE, inc_warmup = TRUE, pars = parnames[A_index])
  fittedtheta[,lp,]      = extract(fit,permuted = FALSE, inc_warmup = TRUE, pars = parnames[theta_index])
  fittedsigma_obs[,lp,]  = extract(fit,permuted = FALSE, inc_warmup = TRUE, pars = parnames[sigma_obs_index])
  fittedsigma_proc[,lp,] = extract(fit,permuted = FALSE, inc_warmup = TRUE, pars = parnames[sigma_proc_index])
  fittedLcorr[,lp,]      = extract(fit,permuted = FALSE, inc_warmup = TRUE, pars = parnames[L_corr_index])
  fittedLproc[,lp,]      = extract(fit,permuted = FALSE, inc_warmup = TRUE, pars = parnames[L_proc_index])
  fittedlp__[,lp,]       = extract(fit,permuted = FALSE, inc_warmup = TRUE, pars = parnames[lp__index])
  fittedX[,lp,]          = extract(fit,permuted = FALSE, inc_warmup = TRUE, pars = parnames[X_index])
  
}

```

Diagnostics:

Total compute time: `r round(runtime,2)` hours.

As a sanity check, just make sure that all of the median log posteriors  are pretty close to each other.  If a subset are more than like 2% worse than others, consdier if they are sampling from different places and/or should be discarded.

```{r}
#| cache: true

niter = 60000
apply(fittedlp__[-c(1:burnin),,],2,median)

```



Consider the Gelman-Rubin Diagnostics to see if the within chain variability is similar to the between chain variability.

Make a mega matrix, but also consider little pieces later for diagnostics:

```{r}
#| code-fold: true


# reconstruct the mattrices to be easier to plot
# then slap on some names
combo_fittedA_mat           = cbind(matrix(aperm(fittedA[-c(1:burnin), , ],            c(1, 2, 3)),nrow = dim(fittedA)[1]*dim(fittedA)[2],                   ncol = dim(fittedA)[3]),          iter = rep((1+burnin):niter, dim(fittedA)[2]),         chain = rep(1:dim(fittedA)[2],           each = burnin)); colnames(combo_fittedA_mat)            = c(parnames[A_index], "iter",  "chain")
combo_fittedtheta_mat       = cbind(matrix(aperm(fittedtheta[-c(1:burnin), , ],        c(1, 2, 3)),nrow = dim(fittedtheta)[1]*dim(fittedtheta)[2],           ncol = dim(fittedtheta)[3]),      iter = rep((1+burnin):niter, dim(fittedtheta)[2]),     chain = rep(1:dim(fittedtheta)[2],       each = burnin)); colnames(combo_fittedtheta_mat)            = c(parnames[theta_index], "iter",  "chain")
combo_fittedsigma_obs_mat   = cbind(matrix(aperm(fittedsigma_obs[-c(1:burnin), , ],    c(1, 2   )),nrow = dim(fittedsigma_obs)[1]*dim(fittedsigma_obs)[2],   ncol = dim(fittedsigma_obs)[3]),  iter = rep((1+burnin):niter, dim(fittedsigma_obs)[2]), chain = rep(1:dim(fittedsigma_obs)[2],   each = burnin)); colnames(combo_fittedsigma_obs_mat)            = c(parnames[sigma_obs_index], "iter",  "chain")
combo_fittedsigma_proc_mat  = cbind(matrix(aperm(fittedsigma_proc[-c(1:burnin), , ],   c(1, 2, 3)),nrow = dim(fittedsigma_proc)[1]*dim(fittedsigma_proc)[2], ncol = dim(fittedsigma_proc)[3]), iter = rep((1+burnin):niter, dim(fittedsigma_proc)[2]),chain = rep(1:dim(fittedsigma_proc)[2],  each = burnin)); colnames(combo_fittedsigma_proc_mat)            = c(parnames[sigma_proc_index], "iter",  "chain")
combo_fittedLcorr_mat       = cbind(matrix(aperm(fittedLcorr[-c(1:burnin), , ],        c(1, 2, 3)),nrow = dim(fittedLcorr)[1]*dim(fittedLcorr)[2],           ncol = dim(fittedLcorr)[3]),      iter = rep((1+burnin):niter, dim(fittedLcorr)[2]),     chain = rep(1:dim(fittedLcorr)[2],       each = burnin)); colnames(combo_fittedLcorr_mat)            = c(parnames[L_corr_index], "iter",  "chain")
combo_fittedLproc_mat       = cbind(matrix(aperm(fittedLproc[-c(1:burnin), , ],        c(1, 2, 3)),nrow = dim(fittedLproc)[1]*dim(fittedLproc)[2],           ncol = dim(fittedLproc)[3]),      iter = rep((1+burnin):niter, dim(fittedLproc)[2]),     chain = rep(1:dim(fittedLproc)[2],       each = burnin)); colnames(combo_fittedLproc_mat)            = c(parnames[L_proc_index], "iter",  "chain")
combo_fittedlp___mat        = cbind(matrix(aperm(fittedlp__[-c(1:burnin), , ],         c(1, 2   )),nrow = dim(fittedlp__)[1]*dim(fittedlp__)[2],             ncol = dim(fittedlp__)[3]),       iter = rep((1+burnin):niter, dim(fittedlp__)[2]),      chain = rep(1:dim(fittedlp__)[2],        each = burnin)); colnames(combo_fittedlp___mat)            = c(parnames[lp__index], "iter",  "chain")
combo_fittedX_mat           = cbind(matrix(aperm(fittedX[-c(1:burnin), , ],            c(1, 2, 3)),nrow = dim(fittedX)[1]*dim(fittedX)[2],                   ncol = dim(fittedX)[3]),          iter = rep((1+burnin):niter, dim(fittedX)[2]),         chain = rep(1:dim(fittedX)[2],           each = burnin)); colnames(combo_fittedX_mat)            = c(parnames[X_index], "iter",  "chain")
```




::: {.callout-note collapse=true title="log posterior trace"}
```{r}
# do some thinning to make the traceplots plots faster
combo_fittedlp___mat[seq(1,nrow(combo_fittedlp___mat),by = 10),] |> 
  as_tibble()|>
  pivot_longer(cols = -c("iter","chain"), names_to = "parameter", values_to = "sample")|>
  ggplot(aes(x = iter,y = sample, colour = as.factor(chain)))+
  geom_line(alpha = .8)


```

:::

::: {.callout-note collapse=true title="A"}







```{r}

# Create a list of mcmc objects, one for each chain
mcmc_chains_list <- lapply(1:dim(fittedA)[2], function(i) {
  # Extract the i-th chain (across the second dimension)
  # and convert it to an mcmc object
  mcmc(fittedA[-c(1:burnin),i,])
})
# Combine the list of mcmc objects into an mcmc.list object
mclist <- as.mcmc.list(mcmc_chains_list)
gelman.diag(mclist)

# do some heavy thinning to make the traceplots plots faster
stacked = combo_fittedA_mat[seq(1,nrow(combo_fittedA_mat), by = 50),] |>
  as_tibble()|>
  pivot_longer(cols = -c("iter","chain"), names_to = "parameter", values_to = "sample")

for(country in 1:length(countries2use)){
  stacked = stacked|> mutate(
    parameter = str_replace_all(parameter,
                                pattern = paste0("A\\[",country),
                                replacement = paste0("A\\[",countries2use[country])))
}
stacked |>   ggplot(aes(x = iter,y = sample, colour = as.factor(chain)))+
  geom_line(alpha = .3)+
  facet_wrap(~parameter)






# do some light thinning to make the density plots faster
stacked = combo_fittedA_mat[seq(1,nrow(combo_fittedA_mat), by = 5),] |>
  as_tibble()|>
  pivot_longer(cols = -c("iter","chain"), names_to = "parameter", values_to = "sample")


for(country in 1:length(countries2use)){
  stacked = stacked|> mutate(
    parameter = str_replace_all(parameter,
                                pattern = paste0("A\\[",country),
                                replacement = paste0("A\\[",countries2use[country])))
}

stacked|>   ggplot(aes(x = sample, colour = as.factor(chain)))+
  geom_density()+
  facet_wrap(~parameter, scales = "free")


```
:::

::: {.callout-note collapse=true title="theta"}

```{r}


# Create a list of mcmc objects, one for each chain
mcmc_chains_list <- lapply(1:dim(fittedtheta)[2], function(i) {
  # Extract the i-th chain (across the second dimension)
  # and convert it to an mcmc object
  mcmc(fittedtheta[-c(1:burnin),i,])
})
# Combine the list of mcmc objects into an mcmc.list object
mclist <- as.mcmc.list(mcmc_chains_list)

gelman.diag(mclist)


# do some heavy thinning to make the traceplots plots faster
stacked = combo_fittedtheta_mat[seq(1,nrow(combo_fittedtheta_mat), by = 100),] |>  
  as_tibble()|>
  pivot_longer(cols = -c("iter","chain"), names_to = "parameter", values_to = "sample")



stacked |>   ggplot(aes(x = iter,y = sample, colour = as.factor(chain)))+
  geom_line(alpha = .3)+
  facet_wrap(~parameter)


# do some light thinning to make the density plots faster
combo_fittedtheta_mat[seq(1,nrow(combo_fittedtheta_mat), by = 5),] |>  
  as_tibble()|>
  pivot_longer(cols = -c("iter","chain"), names_to = "parameter", values_to = "sample")|>
  ggplot(aes(x = sample, colour = as.factor(chain)))+
  geom_density()+
  geom_vline(xintercept = 0)+
  facet_wrap(~parameter, scales = "free_y")



```

:::


::: {.callout-note collapse=true title="observational noise"}

The $\sigma_{\e}$ aka sigma\_obs, the observational noise:



```{r}

# Create a list of mcmc objects, one for each chain
mcmc_chains_list <- lapply(1:dim(fittedsigma_obs)[2], function(i) {
  # Extract the i-th chain (across the second dimension)
  # and convert it to an mcmc object
  mcmc(fittedsigma_obs[-c(1:burnin), i,]) 
})
# Combine the list of mcmc objects into an mcmc.list object
mclist <- as.mcmc.list(mcmc_chains_list)
gelman.diag(mclist)




# do some heavy thinning to make the traceplots plots faster
combo_fittedsigma_obs_mat[seq(1,nrow(combo_fittedsigma_obs_mat),by = 100),] |> 
  as_tibble()|>
  pivot_longer(cols = -c("iter","chain"), names_to = "parameter", values_to = "sample")|>
  ggplot(aes(x = iter,y = sample, colour = as.factor(chain)))+
  geom_line(alpha = .3)





# do some light thinning to make the density plots faster
combo_fittedsigma_obs_mat[seq(1,nrow(combo_fittedsigma_obs_mat),by = 5),] |> 
  as_tibble()|>
  pivot_longer(cols = -c("iter","chain"), names_to = "parameter", values_to = "sample")|>
  ggplot(aes(x = sample, colour = as.factor(chain)))+
  geom_density()



```
:::



::: {.callout-note collapse=true title="process noise"}

The $\sigma_{\delta}$ aka sigma\_proc.  These are the diagonal elements of the process noise.

```{r}

# Create a list of mcmc objects, one for each chain
mcmc_chains_list <- lapply(1:dim(fittedsigma_proc)[2], function(i) {
  # Extract the i-th chain (across the second dimension)
  # and convert it to an mcmc object
  mcmc(fittedsigma_proc[-c(1:burnin), i,]) 
})
# Combine the list of mcmc objects into an mcmc.list object
mclist <- as.mcmc.list(mcmc_chains_list)
gelman.diag(mclist)




# do some heavy thinning to make the traceplots plots faster
stacked = combo_fittedsigma_proc_mat[seq(1,nrow(combo_fittedsigma_proc_mat),by = 100),] |> 
  as_tibble()|>
  pivot_longer(cols = -c("iter","chain"), names_to = "parameter", values_to = "sample")

for(country in 1:length(countries2use)){
  stacked = stacked|> mutate(
    par = str_replace_all(parameter,
                          pattern = paste0("sigma_proc\\[",country),
                          replacement = paste0("sigma_proc\\[",countries2use[country])))
}
stacked |>   
  ggplot(aes(x = iter,y = sample, colour = as.factor(chain)))+
  geom_line(alpha = .3)+
  facet_wrap(~parameter)





# do some light thinning to make the density plots faster
stacked = combo_fittedsigma_proc_mat[seq(1,nrow(combo_fittedsigma_proc_mat),by = 5),] |> 
  as_tibble()|>
  pivot_longer(cols = -c("iter","chain"), names_to = "parameter", values_to = "sample")




for(country in 1:length(countries2use)){
  stacked = stacked|> mutate(
    par = str_replace_all(parameter,
                          pattern = paste0("sigma_proc\\[",country),
                          replacement = paste0("sigma_proc\\[",countries2use[country])))
}

stacked|>   ggplot(aes(x = sample, colour = as.factor(chain)))+
  geom_density()+
  facet_wrap(~parameter, scales = "free")




```

:::




::: {.callout-note collapse=true title="process noise matrix"}
Elements of the correlation matrix from the Cholesky factorization that is used in stan:


```{r}

# Create a list of mcmc objects, one for each chain
mcmc_chains_list <- lapply(1:dim(fittedLcorr)[2], function(i) {
  # Extract the i-th chain (across the second dimension)
  # and convert it to an mcmc object
  mcmc(fittedLcorr[-c(1:burnin), i,]) 
})
# Combine the list of mcmc objects into an mcmc.list object
mclist <- as.mcmc.list(mcmc_chains_list)
gelman.diag(mclist)




# do some heavy thinning to make the traceplots plots faster
combo_fittedLcorr_mat[seq(1,nrow(combo_fittedsigma_proc_mat),by = 100),] |> 
  as_tibble()|>
  pivot_longer(cols = -c("iter","chain"), names_to = "parameter", values_to = "sample") |>   
  ggplot(aes(x = iter,y = sample, colour = as.factor(chain)))+
  geom_line(alpha = .3)+
  geom_hline(yintercept = 0)+
  facet_wrap(~parameter)





# do some light thinning to make the density plots faster
combo_fittedLcorr_mat[seq(1,nrow(combo_fittedsigma_proc_mat),by = 5),] |> 
  as_tibble()|>
  pivot_longer(cols = -c("iter","chain"), names_to = "parameter", values_to = "sample") |>   
  ggplot(aes(x = sample, colour = as.factor(chain)))+
  geom_density()+
  geom_vline(xintercept = 0)+
  facet_wrap(~parameter, scales = "free")
  
  

```




Take a closer look at the correlation structure of the process noise.  These are the lements of $\Sigma_\delta$ in the process evolution:

$$X_{t}= A+\Theta X_{t-1}+ \delta_{t}, \ \ \ where\ \ \ \delta_{t}\sim MVN(0,\Sigma_{\delta}).$$

```{r}
#| cache: true
#| code-fold: true


##### Just go right after L_proc.  It's lower triangle since it's used as 
# a cholesky decomp of the full covariance (and correlation) in the normal transitions



meanies <- lapply(1:dim(fittedLproc)[3], function(i) {
  # Extract the i-th chain (across the second dimension)
  # and convert it to an mcmc object
  mean(fittedLproc[-c(1:burnin), , i]) 
})
meanies = unlist(meanies)
names(meanies) = parnames[L_proc_index]

L_proc_mean_mat = matrix(0, nrow = length(countries2use), ncol =  length(countries2use))
for(rows in 1:length(countries2use)){
  for(cols in 1:(rows)){
    L_proc_mean_mat[rows,cols] = meanies[paste0("L_proc[",rows,",",cols,"]")]
  }
}
# reconstruct by undoing the cholesky decomp: https://mc-stan.org/docs/functions-reference/distributions_over_unbounded_vectors.html#multi-normal-cholesky-fun
Xcovmat = L_proc_mean_mat%*% t(L_proc_mean_mat)

colnames(Xcovmat) = countries2use
rownames(Xcovmat) = countries2use
df <- melt(Xcovmat)
colnames(df) <- c("Row", "Column", "Value")
ggplot(df, aes(x = Column, y = Row, fill = Value)) +
  geom_tile() +
  scale_fill_gradient(name = "Value", low = "white", high = "blue") +
  coord_fixed() +      # Keep square tiles
  labs(title = "Mean Process Covariance between countries")+
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))


diags = 1/sqrt(diag(Xcovmat))
cormat = diag(diags)%*%Xcovmat%*% diag(diags)


colnames(cormat) = countries2use
rownames(cormat) = countries2use
df <- melt(cormat)
colnames(df) <- c("Row", "Column", "Value")
ggplot(df, aes(x = Column, y = Row, fill = Value)) +
  geom_tile() +
  scale_fill_gradient(name = "Value", low = "white", high = "blue") +
  coord_fixed() +      # Keep square tiles
  labs(title = "Mean Process Correlation between countries")+
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))

```





Take a closer look at the relationship between countries.  These are the lements of $\Theta$ in the process evolution:

$$X_{t}= A+\Theta X_{t-1}+ \delta_{t}, \ \ \ where\ \ \ \delta_{t}\sim MVN(0,\Sigma_{\delta}).$$

```{r}
#| cache: true
#| code-fold: true



meanies <- lapply(1:dim(fittedtheta)[3], function(i) {
  # Extract the i-th chain (across the second dimension)
  # and convert it to an mcmc object
  mean(fittedtheta[-c(1:burnin), , i]) 
})
meanies = unlist(meanies)
names(meanies) = parnames[theta_index]





Tmean_mat = matrix(0, nrow = length(countries2use), ncol =  length(countries2use))
for(rows in 1:length(countries2use)){
  for(cols in 1:length(countries2use)){
    Tmean_mat[rows,cols] = meanies[paste0("theta[",rows,",",cols,"]")]
  }
}


colnames(Tmean_mat) = countries2use
rownames(Tmean_mat) = countries2use
df <- melt(Tmean_mat)
colnames(df) <- c("Impact_on", "Lag1", "Value")
ggplot(df, aes(x = Lag1, y = Impact_on, fill = Value)) +
  geom_tile() +
  scale_fill_gradient(name = "Value", low = "white", high = "blue") +
  coord_fixed() +      # Keep square tiles
  labs(title = "Mean Process Impact (Theta)")+
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))


```




:::

::: {.callout-note collapse=true title="Posterior distribution of states"}


```{r}
#| cache: true
#| code-fold: true

Nsam = 1000
Xsamples = combo_fittedX_mat[sample(1:nrow(combo_fittedX_mat), Nsam),]
cols <- adjustcolor(hcl.colors(Nsam, palette = "viridis"), alpha.f = .1)
for(country in 1:length(countries2use)){
  matplot(x = dates_in_use,
          y = t(Xsamples[,grep(colnames(Xsamples),
                               pattern = paste0(country,"\\]"))]),
          type = 'l',  
          main = paste("posterior predictive fit for country",countries2use[country]),
          ylim = c(-.1,.5), ylab = "Topic prevalence", col=cols)
  points(x = dates_in_use[data[,country]!=0],
         data[data[,country]!=0,country], 
         pch = "*", cex = 3, col= "red")
}

```


:::

```{r, cache = TRUE}
#| echo: false
topic_index_for_sections = topic_index_for_sections+1
topic_of_interest = candidate_topics[topic_index_for_sections]
candidate_files_to_load = list.files(pattern = paste0("stan_2chain_dataSS_",topic_of_interest,"_results_Canada_France_Germany_Italy_Japan_United Kingdom_United States_60000"))
```

## Topic `r topic_of_interest`


Results loop over the topics of interest and rely on having run the above code set across different topics.

```{r chunk2334}
#| code-fold: true
runtime = filesloaded = iters_run  =  rep(NA, length(candidate_files_to_load))

# clunky and lazy way to get at the number of time steps for this topic
filename = candidate_files_to_load[1]
load(filename)
parnames = names(fit)

# collect the names of parameters

A_index             = parnames  |> grep(pattern = "^A")
theta_index         = parnames  |> grep(pattern = "theta")
sigma_obs_index     = parnames  |> grep(pattern = "sigma_obs")   
sigma_proc_index    = parnames  |> grep(pattern = "sigma_proc")
# Note that L_corr is the lower triangle cholesky decomposition of the correlation matrix so that the diagonals and upper triangle elements will be all zeros and are not sampled.
L_corr_index = NULL
for(rows in 2:length(A_index)){
  for(cols in 1:(rows-1)){
    L_corr_index = c(L_corr_index,
                     grep(parnames , pattern = paste0("L_corr\\[",rows,",",cols)))
  }
}
L_proc_index        = parnames  |> grep(pattern = "L_proc")
lp__index           = parnames  |> grep(pattern = "lp__")
X_index             = parnames  |> grep(pattern = "^X")

# collect the sampled parameters
# dimensions are: iterations, chains, parameters
fittedA          = array(NA, dim = c(niter,length(candidate_files_to_load),length(A_index)))
fittedtheta      = array(NA, dim = c(niter,length(candidate_files_to_load),length(theta_index)))
fittedsigma_obs  = array(NA, dim = c(niter,length(candidate_files_to_load),length(sigma_obs_index)))
fittedsigma_proc = array(NA, dim = c(niter,length(candidate_files_to_load),length(sigma_proc_index)))
fittedLcorr      = array(NA, dim = c(niter,length(candidate_files_to_load),length(L_corr_index)))
fittedLproc      = array(NA, dim = c(niter,length(candidate_files_to_load),length(L_proc_index)))
fittedlp__       = array(NA, dim = c(niter,length(candidate_files_to_load),length(lp__index)))
fittedX          = array(NA, dim = c(niter,length(candidate_files_to_load),length(X_index)))


for(lp in 1:length(candidate_files_to_load)){
  filename = candidate_files_to_load[lp]
  load(filename)
  runtime[lp]      = as.numeric(elapsed, units = "hours")
  filesloaded[lp]  = filename 
  iters_run[lp]    = niter
  
  fittedA[,lp,]          = extract(fit,permuted = FALSE, inc_warmup = TRUE, pars = parnames[A_index])
  fittedtheta[,lp,]      = extract(fit,permuted = FALSE, inc_warmup = TRUE, pars = parnames[theta_index])
  fittedsigma_obs[,lp,]  = extract(fit,permuted = FALSE, inc_warmup = TRUE, pars = parnames[sigma_obs_index])
  fittedsigma_proc[,lp,] = extract(fit,permuted = FALSE, inc_warmup = TRUE, pars = parnames[sigma_proc_index])
  fittedLcorr[,lp,]      = extract(fit,permuted = FALSE, inc_warmup = TRUE, pars = parnames[L_corr_index])
  fittedLproc[,lp,]      = extract(fit,permuted = FALSE, inc_warmup = TRUE, pars = parnames[L_proc_index])
  fittedlp__[,lp,]       = extract(fit,permuted = FALSE, inc_warmup = TRUE, pars = parnames[lp__index])
  fittedX[,lp,]          = extract(fit,permuted = FALSE, inc_warmup = TRUE, pars = parnames[X_index])
  
}

```

Diagnostics:

Total compute time: `r round(runtime,2)` hours.

As a sanity check, just make sure that all of the median log posteriors  are pretty close to each other.  If a subset are more than like 2% worse than others, consdier if they are sampling from different places and/or should be discarded.

```{r}
#| cache: true

niter = 60000
apply(fittedlp__[-c(1:burnin),,],2,median)

```



Consider the Gelman-Rubin Diagnostics to see if the within chain variability is similar to the between chain variability.

Make a mega matrix, but also consider little pieces later for diagnostics:

```{r}
#| code-fold: true


# reconstruct the mattrices to be easier to plot
# then slap on some names
combo_fittedA_mat           = cbind(matrix(aperm(fittedA[-c(1:burnin), , ],            c(1, 2, 3)),nrow = dim(fittedA)[1]*dim(fittedA)[2],                   ncol = dim(fittedA)[3]),          iter = rep((1+burnin):niter, dim(fittedA)[2]),         chain = rep(1:dim(fittedA)[2],           each = burnin)); colnames(combo_fittedA_mat)            = c(parnames[A_index], "iter",  "chain")
combo_fittedtheta_mat       = cbind(matrix(aperm(fittedtheta[-c(1:burnin), , ],        c(1, 2, 3)),nrow = dim(fittedtheta)[1]*dim(fittedtheta)[2],           ncol = dim(fittedtheta)[3]),      iter = rep((1+burnin):niter, dim(fittedtheta)[2]),     chain = rep(1:dim(fittedtheta)[2],       each = burnin)); colnames(combo_fittedtheta_mat)            = c(parnames[theta_index], "iter",  "chain")
combo_fittedsigma_obs_mat   = cbind(matrix(aperm(fittedsigma_obs[-c(1:burnin), , ],    c(1, 2   )),nrow = dim(fittedsigma_obs)[1]*dim(fittedsigma_obs)[2],   ncol = dim(fittedsigma_obs)[3]),  iter = rep((1+burnin):niter, dim(fittedsigma_obs)[2]), chain = rep(1:dim(fittedsigma_obs)[2],   each = burnin)); colnames(combo_fittedsigma_obs_mat)            = c(parnames[sigma_obs_index], "iter",  "chain")
combo_fittedsigma_proc_mat  = cbind(matrix(aperm(fittedsigma_proc[-c(1:burnin), , ],   c(1, 2, 3)),nrow = dim(fittedsigma_proc)[1]*dim(fittedsigma_proc)[2], ncol = dim(fittedsigma_proc)[3]), iter = rep((1+burnin):niter, dim(fittedsigma_proc)[2]),chain = rep(1:dim(fittedsigma_proc)[2],  each = burnin)); colnames(combo_fittedsigma_proc_mat)            = c(parnames[sigma_proc_index], "iter",  "chain")
combo_fittedLcorr_mat       = cbind(matrix(aperm(fittedLcorr[-c(1:burnin), , ],        c(1, 2, 3)),nrow = dim(fittedLcorr)[1]*dim(fittedLcorr)[2],           ncol = dim(fittedLcorr)[3]),      iter = rep((1+burnin):niter, dim(fittedLcorr)[2]),     chain = rep(1:dim(fittedLcorr)[2],       each = burnin)); colnames(combo_fittedLcorr_mat)            = c(parnames[L_corr_index], "iter",  "chain")
combo_fittedLproc_mat       = cbind(matrix(aperm(fittedLproc[-c(1:burnin), , ],        c(1, 2, 3)),nrow = dim(fittedLproc)[1]*dim(fittedLproc)[2],           ncol = dim(fittedLproc)[3]),      iter = rep((1+burnin):niter, dim(fittedLproc)[2]),     chain = rep(1:dim(fittedLproc)[2],       each = burnin)); colnames(combo_fittedLproc_mat)            = c(parnames[L_proc_index], "iter",  "chain")
combo_fittedlp___mat        = cbind(matrix(aperm(fittedlp__[-c(1:burnin), , ],         c(1, 2   )),nrow = dim(fittedlp__)[1]*dim(fittedlp__)[2],             ncol = dim(fittedlp__)[3]),       iter = rep((1+burnin):niter, dim(fittedlp__)[2]),      chain = rep(1:dim(fittedlp__)[2],        each = burnin)); colnames(combo_fittedlp___mat)            = c(parnames[lp__index], "iter",  "chain")
combo_fittedX_mat           = cbind(matrix(aperm(fittedX[-c(1:burnin), , ],            c(1, 2, 3)),nrow = dim(fittedX)[1]*dim(fittedX)[2],                   ncol = dim(fittedX)[3]),          iter = rep((1+burnin):niter, dim(fittedX)[2]),         chain = rep(1:dim(fittedX)[2],           each = burnin)); colnames(combo_fittedX_mat)            = c(parnames[X_index], "iter",  "chain")
```




::: {.callout-note collapse=true title="log posterior trace"}
```{r}
# do some thinning to make the traceplots plots faster
combo_fittedlp___mat[seq(1,nrow(combo_fittedlp___mat),by = 10),] |> 
  as_tibble()|>
  pivot_longer(cols = -c("iter","chain"), names_to = "parameter", values_to = "sample")|>
  ggplot(aes(x = iter,y = sample, colour = as.factor(chain)))+
  geom_line(alpha = .8)


```

:::

::: {.callout-note collapse=true title="A"}







```{r}

# Create a list of mcmc objects, one for each chain
mcmc_chains_list <- lapply(1:dim(fittedA)[2], function(i) {
  # Extract the i-th chain (across the second dimension)
  # and convert it to an mcmc object
  mcmc(fittedA[-c(1:burnin),i,])
})
# Combine the list of mcmc objects into an mcmc.list object
mclist <- as.mcmc.list(mcmc_chains_list)
gelman.diag(mclist)

# do some heavy thinning to make the traceplots plots faster
stacked = combo_fittedA_mat[seq(1,nrow(combo_fittedA_mat), by = 50),] |>
  as_tibble()|>
  pivot_longer(cols = -c("iter","chain"), names_to = "parameter", values_to = "sample")

for(country in 1:length(countries2use)){
  stacked = stacked|> mutate(
    parameter = str_replace_all(parameter,
                                pattern = paste0("A\\[",country),
                                replacement = paste0("A\\[",countries2use[country])))
}
stacked |>   ggplot(aes(x = iter,y = sample, colour = as.factor(chain)))+
  geom_line(alpha = .3)+
  facet_wrap(~parameter)






# do some light thinning to make the density plots faster
stacked = combo_fittedA_mat[seq(1,nrow(combo_fittedA_mat), by = 5),] |>
  as_tibble()|>
  pivot_longer(cols = -c("iter","chain"), names_to = "parameter", values_to = "sample")


for(country in 1:length(countries2use)){
  stacked = stacked|> mutate(
    parameter = str_replace_all(parameter,
                                pattern = paste0("A\\[",country),
                                replacement = paste0("A\\[",countries2use[country])))
}

stacked|>   ggplot(aes(x = sample, colour = as.factor(chain)))+
  geom_density()+
  facet_wrap(~parameter, scales = "free")



```
:::

::: {.callout-note collapse=true title="theta"}

```{r}


# Create a list of mcmc objects, one for each chain
mcmc_chains_list <- lapply(1:dim(fittedtheta)[2], function(i) {
  # Extract the i-th chain (across the second dimension)
  # and convert it to an mcmc object
  mcmc(fittedtheta[-c(1:burnin),i,])
})
# Combine the list of mcmc objects into an mcmc.list object
mclist <- as.mcmc.list(mcmc_chains_list)

gelman.diag(mclist)


# do some heavy thinning to make the traceplots plots faster
stacked = combo_fittedtheta_mat[seq(1,nrow(combo_fittedtheta_mat), by = 100),] |>  
  as_tibble()|>
  pivot_longer(cols = -c("iter","chain"), names_to = "parameter", values_to = "sample")



stacked |>   ggplot(aes(x = iter,y = sample, colour = as.factor(chain)))+
  geom_line(alpha = .3)+
  facet_wrap(~parameter)


# do some light thinning to make the density plots faster
combo_fittedtheta_mat[seq(1,nrow(combo_fittedtheta_mat), by = 5),] |>  
  as_tibble()|>
  pivot_longer(cols = -c("iter","chain"), names_to = "parameter", values_to = "sample")|>
  ggplot(aes(x = sample, colour = as.factor(chain)))+
  geom_density()+
  geom_vline(xintercept = 0)+
  facet_wrap(~parameter, scales = "free_y")



```

:::


::: {.callout-note collapse=true title="observational noise"}

The $\sigma_{\e}$ aka sigma\_obs, the observational noise:



```{r}

# Create a list of mcmc objects, one for each chain
mcmc_chains_list <- lapply(1:dim(fittedsigma_obs)[2], function(i) {
  # Extract the i-th chain (across the second dimension)
  # and convert it to an mcmc object
  mcmc(fittedsigma_obs[-c(1:burnin), i,]) 
})
# Combine the list of mcmc objects into an mcmc.list object
mclist <- as.mcmc.list(mcmc_chains_list)
gelman.diag(mclist)




# do some heavy thinning to make the traceplots plots faster
combo_fittedsigma_obs_mat[seq(1,nrow(combo_fittedsigma_obs_mat),by = 100),] |> 
  as_tibble()|>
  pivot_longer(cols = -c("iter","chain"), names_to = "parameter", values_to = "sample")|>
  ggplot(aes(x = iter,y = sample, colour = as.factor(chain)))+
  geom_line(alpha = .3)





# do some light thinning to make the density plots faster
combo_fittedsigma_obs_mat[seq(1,nrow(combo_fittedsigma_obs_mat),by = 5),] |> 
  as_tibble()|>
  pivot_longer(cols = -c("iter","chain"), names_to = "parameter", values_to = "sample")|>
  ggplot(aes(x = sample, colour = as.factor(chain)))+
  geom_density()



```
:::



::: {.callout-note collapse=true title="process noise"}

The $\sigma_{\delta}$ aka sigma\_proc.  These are the diagonal elements of the process noise.

```{r}

# Create a list of mcmc objects, one for each chain
mcmc_chains_list <- lapply(1:dim(fittedsigma_proc)[2], function(i) {
  # Extract the i-th chain (across the second dimension)
  # and convert it to an mcmc object
  mcmc(fittedsigma_proc[-c(1:burnin), i,]) 
})
# Combine the list of mcmc objects into an mcmc.list object
mclist <- as.mcmc.list(mcmc_chains_list)
gelman.diag(mclist)




# do some heavy thinning to make the traceplots plots faster
stacked = combo_fittedsigma_proc_mat[seq(1,nrow(combo_fittedsigma_proc_mat),by = 100),] |> 
  as_tibble()|>
  pivot_longer(cols = -c("iter","chain"), names_to = "parameter", values_to = "sample")

for(country in 1:length(countries2use)){
  stacked = stacked|> mutate(
    par = str_replace_all(parameter,
                          pattern = paste0("sigma_proc\\[",country),
                          replacement = paste0("sigma_proc\\[",countries2use[country])))
}
stacked |>   
  ggplot(aes(x = iter,y = sample, colour = as.factor(chain)))+
  geom_line(alpha = .3)+
  facet_wrap(~parameter)





# do some light thinning to make the density plots faster
stacked = combo_fittedsigma_proc_mat[seq(1,nrow(combo_fittedsigma_proc_mat),by = 5),] |> 
  as_tibble()|>
  pivot_longer(cols = -c("iter","chain"), names_to = "parameter", values_to = "sample")




for(country in 1:length(countries2use)){
  stacked = stacked|> mutate(
    par = str_replace_all(parameter,
                          pattern = paste0("sigma_proc\\[",country),
                          replacement = paste0("sigma_proc\\[",countries2use[country])))
}

stacked|>   ggplot(aes(x = sample, colour = as.factor(chain)))+
  geom_density()+
  facet_wrap(~parameter, scales = "free")



```

:::




::: {.callout-note collapse=true title="process noise matrix"}
Elements of the correlation matrix from the Cholesky factorization that is used in stan:


```{r}

# Create a list of mcmc objects, one for each chain
mcmc_chains_list <- lapply(1:dim(fittedLcorr)[2], function(i) {
  # Extract the i-th chain (across the second dimension)
  # and convert it to an mcmc object
  mcmc(fittedLcorr[-c(1:burnin), i,]) 
})
# Combine the list of mcmc objects into an mcmc.list object
mclist <- as.mcmc.list(mcmc_chains_list)
gelman.diag(mclist)




# do some heavy thinning to make the traceplots plots faster
combo_fittedLcorr_mat[seq(1,nrow(combo_fittedsigma_proc_mat),by = 100),] |> 
  as_tibble()|>
  pivot_longer(cols = -c("iter","chain"), names_to = "parameter", values_to = "sample") |>   
  ggplot(aes(x = iter,y = sample, colour = as.factor(chain)))+
  geom_line(alpha = .3)+
  geom_hline(yintercept = 0)+
  facet_wrap(~parameter)





# do some light thinning to make the density plots faster
combo_fittedLcorr_mat[seq(1,nrow(combo_fittedsigma_proc_mat),by = 5),] |> 
  as_tibble()|>
  pivot_longer(cols = -c("iter","chain"), names_to = "parameter", values_to = "sample") |>   
  ggplot(aes(x = sample, colour = as.factor(chain)))+
  geom_density()+
  geom_vline(xintercept = 0)+
  facet_wrap(~parameter, scales = "free")
  
  

```




Take a closer look at the correlation structure of the process noise.  These are the lements of $\Sigma_\delta$ in the process evolution:

$$X_{t}= A+\Theta X_{t-1}+ \delta_{t}, \ \ \ where\ \ \ \delta_{t}\sim MVN(0,\Sigma_{\delta}).$$

```{r}
#| cache: true
#| code-fold: true


##### Just go right after L_proc.  It's lower triangle since it's used as 
# a cholesky decomp of the full covariance (and correlation) in the normal transitions



meanies <- lapply(1:dim(fittedLproc)[3], function(i) {
  # Extract the i-th chain (across the second dimension)
  # and convert it to an mcmc object
  mean(fittedLproc[-c(1:burnin), , i]) 
})
meanies = unlist(meanies)
names(meanies) = parnames[L_proc_index]

L_proc_mean_mat = matrix(0, nrow = length(countries2use), ncol =  length(countries2use))
for(rows in 1:length(countries2use)){
  for(cols in 1:(rows)){
    L_proc_mean_mat[rows,cols] = meanies[paste0("L_proc[",rows,",",cols,"]")]
  }
}
# reconstruct by undoing the cholesky decomp: https://mc-stan.org/docs/functions-reference/distributions_over_unbounded_vectors.html#multi-normal-cholesky-fun
Xcovmat = L_proc_mean_mat%*% t(L_proc_mean_mat)

colnames(Xcovmat) = countries2use
rownames(Xcovmat) = countries2use
df <- melt(Xcovmat)
colnames(df) <- c("Row", "Column", "Value")
ggplot(df, aes(x = Column, y = Row, fill = Value)) +
  geom_tile() +
  scale_fill_gradient(name = "Value", low = "white", high = "blue") +
  coord_fixed() +      # Keep square tiles
  labs(title = "Mean Process Covariance between countries")+
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))


diags = 1/sqrt(diag(Xcovmat))
cormat = diag(diags)%*%Xcovmat%*% diag(diags)


colnames(cormat) = countries2use
rownames(cormat) = countries2use
df <- melt(cormat)
colnames(df) <- c("Row", "Column", "Value")
ggplot(df, aes(x = Column, y = Row, fill = Value)) +
  geom_tile() +
  scale_fill_gradient(name = "Value", low = "white", high = "blue") +
  coord_fixed() +      # Keep square tiles
  labs(title = "Mean Process Correlation between countries")+
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))

```





Take a closer look at the relationship between countries.  These are the lements of $\Theta$ in the process evolution:

$$X_{t}= A+\Theta X_{t-1}+ \delta_{t}, \ \ \ where\ \ \ \delta_{t}\sim MVN(0,\Sigma_{\delta}).$$

```{r}
#| cache: true
#| code-fold: true



meanies <- lapply(1:dim(fittedtheta)[3], function(i) {
  # Extract the i-th chain (across the second dimension)
  # and convert it to an mcmc object
  mean(fittedtheta[-c(1:burnin), , i]) 
})
meanies = unlist(meanies)
names(meanies) = parnames[theta_index]





Tmean_mat = matrix(0, nrow = length(countries2use), ncol =  length(countries2use))
for(rows in 1:length(countries2use)){
  for(cols in 1:length(countries2use)){
    Tmean_mat[rows,cols] = meanies[paste0("theta[",rows,",",cols,"]")]
  }
}


colnames(Tmean_mat) = countries2use
rownames(Tmean_mat) = countries2use
df <- melt(Tmean_mat)
colnames(df) <- c("Impact_on", "Lag1", "Value")
ggplot(df, aes(x = Lag1, y = Impact_on, fill = Value)) +
  geom_tile() +
  scale_fill_gradient(name = "Value", low = "white", high = "blue") +
  coord_fixed() +      # Keep square tiles
  labs(title = "Mean Process Impact (Theta)")+
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))


```




:::

::: {.callout-note collapse=true title="Posterior distribution of states"}


```{r}
#| cache: true
#| code-fold: true

Nsam = 1000
Xsamples = combo_fittedX_mat[sample(1:nrow(combo_fittedX_mat), Nsam),]
cols <- adjustcolor(hcl.colors(Nsam, palette = "viridis"), alpha.f = .1)
for(country in 1:length(countries2use)){
  matplot(x = dates_in_use,
          y = t(Xsamples[,grep(colnames(Xsamples),
                               pattern = paste0(country,"\\]"))]),
          type = 'l',  
          main = paste("posterior predictive fit for country",countries2use[country]),
          ylim = c(-.1,.5), ylab = "Topic prevalence", col=cols)
  points(x = dates_in_use[data[,country]!=0],
         data[data[,country]!=0,country], 
         pch = "*", cex = 3, col= "red")
}

```


:::

::::


## Running this on real data:

- Code could be made more efficient using Sequential Monte Carlo. Stan is generally pretty good though.
- Another reasonable solution is to go fully frequentist and optimize the marginal likelihood by integrating over the nuissance states **X** using Integrated Nested Laplace Approximation.  The package **tmb** is designed to do this quickly.
- Use the script to extract the R code, because this will take a while to run. 

```{r purl}
#| eval: false

knitr::purl("DATA_stan_state_space_model.qmd")


```
